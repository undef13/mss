{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"SpliFFT","text":"<p>Lightweight utilities for music source separation.</p> <p>This library is a ground-up rewrite of the zfturbo's MSST repo, with a strong focus on robustness, simplicity and extensibility. While it is a fantastic collection of models and training scripts, this rewrite adopts a different architecture to address common pain points in research code.</p> <p>Key principles:</p> <ul> <li>Configuration as code: pydantic models are used instead of untyped dictionaries or <code>ConfigDict</code>. this provides static type safety, runtime data validation, IDE autocompletion, and a single, clear source of truth for all parameters.</li> <li>Data-oriented and functional core: complex class hierarchies and inheritance are avoided. the codebase is built on plain data structures (like <code>dataclasses</code>) and pure, stateless functions.</li> <li>Semantic typing as documentation: we leverage Python's type system to convey intent. types like <code>RawAudioTensor</code> vs. <code>NormalizedAudioTensor</code> make function signatures self-documenting, reducing the need for verbose comments and ensuring correctness.</li> <li>Extensibility without modification: new models can be integrated from external packages without altering the core library. the dynamic model loading system allows easy plug-and-play adhering to the open/closed principle.</li> </ul> <p>\u26a0\ufe0f This is pre-alpha software, expect significant breaking changes.</p>"},{"location":"#features-and-roadmap","title":"Features and Roadmap","text":"<p>Short term (high priority)</p> <ul> <li> a robust, typed JSON configuration system powered by <code>pydantic</code></li> <li> inferencing:<ul> <li> normalization and denormalization</li> <li> chunk generation: vectorized with <code>unfold</code></li> <li> chunk stitching: vectorized overlap-add with <code>fold</code></li> <li> flexible ruleset for stem deriving: add/subtract model outputs or any intermediate output (e.g., creating an <code>instrumental</code> track by subtracting <code>vocals</code> from the <code>mixture</code>).</li> </ul> </li> <li> web-based docs: generated with <code>mkdocs</code> with excellent crossrefs.</li> <li> simple CLI for inferencing on a directory of audio files</li> <li> <code>BS-Roformer</code>: ensure bit-for-bit equivalence in pytorch and strive for max perf.</li> <li> initial fp16 support</li> <li> support <code>coremltools</code> and <code>torch.compile</code><ul> <li> handroll complex multiplication implementation</li> <li> handroll stft in forward pass</li> </ul> </li> <li> port additional SOTA models from MSST (e.g. Mel Roformer, SCNet)</li> <li> directly support popular models (e.g. by @unwa, gabox, by @becruily)</li> <li> model registry with simple file-based cache</li> <li> evals: SDR, bleedless, fullness, etc.</li> <li> proper benchmarking (MFU, memory...)</li> <li> datasets: MUSDB18-HQ, moises</li> </ul> <p>Medium term</p> <ul> <li> simple web-based GUI with FastAPI and SolidJS.</li> <li> Jupyter notebook</li> </ul> <p>Long term (low priority)</p> <ul> <li> data augmentation</li> <li> implement a complete, configurable training loop</li> <li> <code>max</code> kernels</li> </ul> <p>Contributing: PRs are very welcome!</p>"},{"location":"#installation-usage","title":"Installation &amp; Usage","text":"<ul> <li>I just want to run it</li> <li>I want to add it as a library to my Python project</li> <li>I want to hack around</li> </ul> <p>Documentation on the config (amongst other details) can be found here</p>"},{"location":"#cli","title":"CLI","text":"<p>There are three steps. You do not need to have Python installed.</p> <ol> <li> <p>Install <code>uv</code> if you haven't already. It is an awesome Python package and library manager with pip comptability.     <pre><code># Linux / MacOS\nwget -qO- https://astral.sh/uv/install.sh | sh\n# Windows\npowershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n</code></pre></p> </li> <li> <p>Open a new terminal and install the latest stable PyPI release as a tool. It will install the Python interpreter, all necessary packages and add the <code>splifft</code> executable to your <code>PATH</code>:     <pre><code>uv tool install \"splifft[config,inference,cli]\"\n</code></pre> I want the latest bleeding-edge version <p>This directly pulls from the <code>main</code> branch, which may be unstable: <pre><code>uv tool install \"git+https://github.com/undef13/splifft.git[config,inference,cli]\"\n</code></pre> </p> <li> <p>Go into a new directory and place the model checkpoint and configuration inside it. Assuming your current directory has this structure (doesn't have to be exactly this):</p> <p> Minimal reproduction: with example audio from YouTube <p><pre><code>uv tool install yt-dlp\nyt-dlp -f bestaudio -o data/audio/input/3BFTio5296w.flac 3BFTio5296w\nwget -P data/models/ https://huggingface.co/undef13/splifft/resolve/main/roformer-fp16.pt?download=true\nwget -P data/config/ https://raw.githubusercontent.com/undef13/splifft/refs/heads/main/data/config/bs_roformer.json\n</code></pre> </p> <pre><code>.\n\u2514\u2500\u2500 data\n    \u251c\u2500\u2500 audio\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 input\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 3BFTio5296w.flac\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 output\n    \u251c\u2500\u2500 config\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 bs_roformer.json\n    \u2514\u2500\u2500 models\n        \u2514\u2500\u2500 roformer-fp16.pt\n</code></pre> <p>Run: <pre><code>splifft separate data/audio/input/3BFTio5296w.flac --config data/config/bs_roformer.json --checkpoint data/models/roformer-fp16.pt\n</code></pre> Console output <p><pre><code>[00:00:41] INFO     using device=device(type='cuda')                                                 __main__.py:111\n           INFO     loading configuration from                                                       __main__.py:113\n                    config_path=PosixPath('data/config/bs_roformer.json')                                           \n           INFO     loading model metadata `BSRoformer` from module `splifft.models.bs_roformer`     __main__.py:126\n[00:00:42] INFO     loading weights from checkpoint_path=PosixPath('data/models/roformer-fp16.pt')   __main__.py:127\n           INFO     processing audio file:                                                           __main__.py:135\n                    mixture_path=PosixPath('data/audio/input/3BFTio5296w.flac')                                     \n\u2819 processing chunks... \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u257a\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501  25% 0:00:10 (bs=4 \u2022 cuda \u2022 float16)\n[00:00:56] INFO     wrote stem `bass` to data/audio/output/3BFTio5296w/bass.flac                     __main__.py:158\n           INFO     wrote stem `drums` to data/audio/output/3BFTio5296w/drums.flac                   __main__.py:158\n           INFO     wrote stem `other` to data/audio/output/3BFTio5296w/other.flac                   __main__.py:158\n[00:00:57] INFO     wrote stem `vocals` to data/audio/output/3BFTio5296w/vocals.flac                 __main__.py:158\n           INFO     wrote stem `guitar` to data/audio/output/3BFTio5296w/guitar.flac                 __main__.py:158\n           INFO     wrote stem `piano` to data/audio/output/3BFTio5296w/piano.flac                   __main__.py:158\n[00:00:58] INFO     wrote stem `instrumental` to data/audio/output/3BFTio5296w/instrumental.flac     __main__.py:158\n           INFO     wrote stem `drums_and_bass` to data/audio/output/3BFTio5296w/drums_and_bass.flac __main__.py:158\n</code></pre> </p> <p>To update the tool:</p> <pre><code>uv tool upgrade splifft --force-reinstall\n</code></pre>"},{"location":"#library","title":"Library","text":"<p>Add <code>splifft</code> to your project:</p> <pre><code># latest pypi version\nuv add splifft\n# latest bleeding edge\nuv add git+https://github.com/undef13/splifft.git\n</code></pre> <p>This will install the absolutely minimal core dependencies used under the <code>src/splifft/models</code> directory. Higher level components, e.g. inference, training or CLI components must be installed via optional depedencies, as specified in the <code>project.optional-dependencies</code> section of <code>pyproject.toml</code>, for example:</p> <pre><code># enable the built-in configuration, inference and CLI\nuv add \"splifft[config,inference,cli]\"\n</code></pre> <p>This will install <code>splifft</code> in your venv.</p>"},{"location":"#development","title":"Development","text":"<p>If you'd like to make local changes, it is recommended to enable all optional and developer group dependencies:</p> <pre><code>git clone https://github.com/undef13/splifft.git\ncd splifft\nuv venv\nuv sync --all-extras --all-groups\n</code></pre> <p>You may also want to use <code>--editable</code> with <code>sync</code>. Check your code:</p> <pre><code># lint &amp; format\njust fmt\n# build &amp; host documentation\njust docs\n</code></pre> <p>Format your code: <pre><code>just fmt\n</code></pre></p> <p>This repo is no longer compatible with zfturbo's repo. The last version that does so is <code>v0.0.1</code>. To pin a specific version in <code>uv</code>, change your <code>pyproject.toml</code>:</p> <pre><code>[tool.uv.sources]\nsplifft = { git = \"https://github.com/undef13/splifft.git\", rev = \"287235e520f3bb927b58f9f53749fe3ccc248fac\" }\n</code></pre>"},{"location":"#mojo","title":"Mojo","text":"<p>While the primary goal is just to have minimalist PyTorch-based inference engine, I will be using this project as an opportunity to learn more about heterogenous computing, particularly with the Mojo language. The ultimate goal will be to understand to what extent can its compile-time metaprogramming and explicit memory layout control be used.</p> <p>My approach will be incremental and bottom-up: I'll develop, test and benchmark small components against their PyTorch counterparts. The PyTorch implementation will always remain the \"source of truth\", the fully functional baseline and never be removed.</p> <p>TODO:</p> <ul> <li> evaluate <code>pixi</code> in <code>pyproject.toml</code>.</li> <li> use <code>max.torch.CustomOpLibrary</code> to provide a callable from the pytorch side</li> <li> use <code>DeviceContext</code> to interact with the GPU</li> <li> attention</li> <li> use <code>LayoutTensor</code> for QKV</li> <li> rotary embedding</li> <li> feedforward</li> <li> transformer</li> <li> <code>BandSplit</code> &amp; <code>MaskEstimator</code></li> <li> full graph compilation</li> </ul>"},{"location":"concepts/","title":"Concepts & Definitions","text":""},{"location":"concepts/#introduction","title":"Introduction","text":"<p>In the digital world, sound is captured as a discrete sequence of samples, a representation of the original continuous audio signal \\(x(t)\\). We refer to this time-domain data as a <code>RawAudioTensor</code>. This digital signal is defined by several key parameters: its sample rate, number of channels, bit rate and file format.</p> <p>The full range of human hearing is approximately 20-20000 Hz, so according to the Nyquist-Shannon sampling theorem, the minimum sample rate to accurately represent this range is 40000 Hz. Common values are 44100 Hz (CD quality), 48000 Hz (professional audio), and 16000 Hz (voice).</p>"},{"location":"concepts/#the-separation-pipeline","title":"The Separation Pipeline","text":""},{"location":"concepts/#normalization","title":"Normalization","text":"<p>Neural networks perform best when their input data has a consistent statistical distribution. To prepare the audio for the model, we first normalize it. This process transforms the <code>RawAudioTensor</code> into a <code>NormalizedAudioTensor</code> with a mean of 0 and a standard deviation of 1. The original statistics (\\(\\mu\\) and \\(\\sigma\\)) are stored in <code>NormalizationStats</code> to be used later for denormalization.</p>"},{"location":"concepts/#time-frequency-transformation","title":"Time-Frequency Transformation","text":"<p>Models usually operates not on raw time-domain samples, but in the time-frequency domain, which reveals the signal's frequency content over time. This is achieved via the Short-Time Fourier Transform (STFT), which converts the 1D audio signal into a 2D complex spectrogram.</p>"},{"location":"concepts/#complex-spectrogram","title":"Complex Spectrogram","text":"<p>The STFT coefficient \\(X[m, k]\\) is a complex number that can be decomposed into:</p> <ul> <li>Magnitude \\(|X[m, k]|\\): Tells us \"how much\" of a frequency is present (i.e., its loudness).</li> <li>Phase \\(\\phi(m, k)\\): Tells us \"how it's aligned\" in time. This is notoriously difficult to model, as it chaotically wraps around from \\(-\\pi\\) to \\(\\pi\\). Human hearing is highly sensitive to phase, which is crucial for sound localization and timbre perception.</li> </ul> <p>Practically, the process involves:</p> <ol> <li>Dividing the audio signal into short, overlapping segments in time (chunks), parameterized by the    hop size \\(H\\)</li> <li>Applying a window function \\(w[n]\\) (e.g.    Hann window) to each chunk to reduce spectral leakage</li> <li>Computing the Fast Fourier Transform (FFT) on each windowed segment to get its complex frequency    spectrum. The FFT size \\(N_\\text{fft}\\) determines the number of frequency    bins.</li> <li>Stacking these spectra to form the 2D complex spectrogram.</li> </ol> <p>This is commonly used as the input to models. The objective of source separation is to approximate an ideal ratio mask or its complex equivalent: \\(\\hat{S}_\\text{source} = M_\\text{complex} \\odot S_\\text{mixture}\\).</p>"},{"location":"concepts/#fft-size","title":"FFT Size","text":"<p>The choice of <code>FftSize</code> presents a fundamental trade-off between the uncertainty in time \\(t\\) and frequency \\(f\\): \\(\\sigma_t \\sigma_f \\ge \\frac{1}{4\\pi}\\)</p> <ul> <li>a short window gives good time resolution, excellent for capturing sharp, percussive sounds (transients).</li> <li>a long window gives good frequency resolution, ideal for separating fine harmonics of tonal instruments.</li> </ul> <p>To address this, some loss functions (e.g. <code>auraloss.MultiResolutionSTFTLoss</code>) calculate the error on spectrograms with multiple FFT sizes, forcing the model to optimize for both transient and tonal accuracy.</p>"},{"location":"concepts/#bands","title":"Bands","text":"<p>Instead of processing every frequency bin independently, we can group them into <code>Bands</code>. This reduces computational complexity and allows the model to learn relationships within frequency regions, which often correspond to musical harmonics. Some models use perceptually-motivated scales like the Mel scale, while others like BS-Roformer use a linear frequency scale and learn their own relevant bandings.</p>"},{"location":"concepts/#chunking-and-inference","title":"Chunking and Inference","text":"<p>Since a full audio track is too large for GPU memory, we process it in overlapping segments. The <code>ChunkSize</code> defines the segment length, while the <code>HopSize</code> dictates the step between them, controlled by the <code>OverlapRatio</code>. This process yields a stream of <code>PaddedChunkedAudioTensor</code> batches, which are fed into the model. The model then outputs a corresponding stream of <code>SeparatedChunkedTensor</code>.</p>"},{"location":"concepts/#stitching-and-post-processing","title":"Stitching and Post-processing","text":"<p>After the model processes each chunk, we must reconstruct the full-length audio. The <code>stitch_chunks</code> function implements the overlap-add method, which applies a <code>WindowTensor</code> to each chunk to ensure an artifact-free reconstruction. The final result is a <code>RawSeparatedTensor</code> for each separated stem.</p> <p>With the separated audio back in the time domain, the final steps are to reverse the normalization using the original <code>NormalizationStats</code> and, optionally, to create new stems (e.g., an \"instrumental\" track) using rules defined in <code>DerivedStemsConfig</code>.</p>"},{"location":"models/","title":"Models","text":"<p>More information on the expected JSON schema for the configuration can be found in splifft.config.Config.</p>"},{"location":"models/#supported-models","title":"Supported models","text":""},{"location":"models/#bs-roformer","title":"BS Roformer","text":"<ul> <li>checkpoint: <code>roformer-fp16.pt</code></li> <li>configuration: <code>bs_roformer.json</code></li> <li><code>config.model</code></li> <li><code>config.model_type = \"bs_roformer\"</code></li> </ul>"},{"location":"models/#visualizations","title":"Visualizations","text":"<p>The following are quick comparisons for the quality of different models, as evaluated on MVSep. Support for running these models are not yet implemented but may come in a future release.</p> InstrumentalVocals <p> 2025-06-11T17:39:03.154243 image/svg+xml Matplotlib v3.10.3, https://matplotlib.org/ 12 14 16 18 sdr 30 35 40 45 bleedless 20 25 30 35 40 fullness 12 13 14 15 16 17 18 sdr 30 35 40 45 bleedless 0 5 10 15 20 25 30 fullness Metric Correlations for `instrum` 7534: unwa inst v1e (raw) 7573: MelBand Roformer Kim | segment size 2048 | overlap 2 | UVR 5.6.1 7768: Mel-Roformer Instrumental model F (by Gabox) 8257: Inst_GaboxFv8 (updated) 8303: inst fv8b 8362: Logic Pro BS-RoFormer 8393: Inst Fv9 / Bv4? </p> <p> 2025-06-11T17:38:59.246903 image/svg+xml Matplotlib v3.10.3, https://matplotlib.org/ 6 8 10 sdr 15 20 25 30 35 40 bleedless 10.0 12.5 15.0 17.5 20.0 fullness 5 6 7 8 9 10 11 sdr 15 20 25 30 35 40 bleedless 0 5 10 15 20 25 30 fullness Metric Correlations for `vocals` 7475: Mel-RoFormer Kimberley | Unwa's Big Beta 5e(mphasis fullnes) (by unwa) 7706: mel_band_roformer_vocals (by becruily) 8093: Big Beta 6X 8265: BS-Roformer Revive2 8337: BS-Roformer Revive3e 8377: BS\u00b2 </p>"},{"location":"tutorial/","title":"Library Tutorial","text":""},{"location":"tutorial/#basic-inference","title":"Basic inference","text":"<p>This example demonstrates the lower level API for inference usecases. In the future, we will have a high level API for convenience.</p> inference.py<pre><code># ruff: noqa: E402\nfrom pathlib import Path\n\nPATH_CONFIG = Path(\"data/config/bs_roformer.json\")\nPATH_CKPT = Path(\"data/models/roformer-fp16.pt\")\nPATH_MIXTURE = Path(\"data/audio/input/3BFTio5296w.flac\")\n\n# 1. parse + validate a JSON *without* having to import a particular pytorch model.\nfrom splifft.config import Config\n\nconfig = Config.from_file(PATH_CONFIG)\n\n# 2. we now want to *lock in* the configuration to a specific model.\nfrom splifft.models import ModelMetadata\nfrom splifft.models.bs_roformer import BSRoformer, BSRoformerParams\n\nmetadata = ModelMetadata(model_type=\"bs_roformer\", params=BSRoformerParams, model=BSRoformer)\nmodel_params = config.model.to_concrete(metadata.params)\n\n# 3. `metadata` acts as a model builder\nfrom splifft.io import load_weights\n\nmodel = metadata.model(model_params)\nmodel = load_weights(model, PATH_CKPT, device=\"cpu\")\n\n# 4. load audio and run inference by passing dependencies explicitly.\nfrom splifft.inference import run_inference_on_file\nfrom splifft.io import read_audio\n\nmixture = read_audio(\n    PATH_MIXTURE,\n    config.audio_io.target_sample_rate,\n    config.audio_io.force_channels,\n)\nstems = run_inference_on_file(mixture, config, model, model_params)\n\nprint(list(stems.keys()))\n</code></pre>"},{"location":"tutorial/#extending-splifft","title":"Extending <code>splifft</code>","text":"<p><code>splifft</code> is designed to be easily extended without modifying its core.</p> <p>Make sure you have added <code>splifft</code> as a dependency. Assuming your library has this structure: tree /path/to/ext_project<pre><code>\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 scripts\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 main.py\n\u2514\u2500\u2500 src\n    \u2514\u2500\u2500 my_library\n        \u2514\u2500\u2500 models\n            \u251c\u2500\u2500 __init__.py\n            \u2514\u2500\u2500 my_model.py\n</code></pre></p>"},{"location":"tutorial/#1-define-a-new-model","title":"1. Define a new model","text":"<p>Don't do this</p> <p>A common pattern is to define a model with a huge list of parameters in its <code>__init__</code> method:</p> src/my_library/models/my_model.py<pre><code>from torch import nn\nfrom beartype import beartype\n\nclass MyModel(nn.Module):\n    @beartype\n    def __init__(\n        self,\n        chunk_size: int,\n        output_stem_names: tuple[str, ...],\n        # ... a bunch of args here\n    ):\n        ...\n</code></pre> <p>The problem is that it tightly couples the model's implementation to its configuration. Serializing to/from a JSON file and simultaneously supporting static type checking is a headache.</p> <p>Instead, define a stdlib <code>dataclass</code> separate from the model:</p> src/my_library/models/my_model.py<pre><code>from dataclasses import dataclass\n\nfrom torch import nn\n\nfrom splifft.models import ModelParamsLike\nfrom splifft.types import ChunkSize, ModelInputType, ModelOutputStemName, ModelOutputType\n\n\n@dataclass\nclass MyModelParams(ModelParamsLike):  # (1)!\n    chunk_size: ChunkSize\n    output_stem_names: tuple[ModelOutputStemName, ...]\n    # ... any other config your model needs\n    @property\n    def input_type(self) -&gt; ModelInputType:\n        return \"waveform\"\n    @property\n    def output_type(self) -&gt; ModelOutputType:\n        return \"waveform\"\n\n\nclass MyModel(nn.Module):\n    def __init__(self, params: MyModelParams):\n        super().__init__()\n        self.params = params\n</code></pre> <ol> <li><code>ModelParamsLike</code> is not a base class to inherit from, but rather a form of structural typing that signals that <code>MyModelParams</code> is compatible with the <code>splifft</code> configuration system. You can remove it if you don't like it.</li> </ol>"},{"location":"tutorial/#2-register-the-model","title":"2. Register the model","text":"<p>With the model and its config defined, our configuration system needs to understand your model.</p> <p>Don't do this</p> <p>A common solution is to define a \"global\" dictionary of available models:</p> src/my_library/models/__init__.py<pre><code>from my_library.models.my_model import MyModelParams, MyModel\n\nMODEL_REGISTRY = {\n    \"my_model\": (MyModel, MyModelParams),\n    # every other model must be added here\n}\n</code></pre> <p>To add a new model, you'd have to modify this central registry. It also forces the import of all models and unwanted dependencies at once.</p> <p>Instead, our configuration system uses a simple <code>ModelMetadata</code> wrapper struct to act as a \"descriptor\" for your model. Create a factory function that defers the imports until its actually needed:</p> src/my_library/models/__init__.py<pre><code>from splifft.models import ModelMetadata\n\n\ndef my_model_metadata():\n    from .my_model import MyModel, MyModelParams\n\n    return ModelMetadata(model_type=\"my_model\", params=MyModelParams, model=MyModel)\n</code></pre> I need to take a user's input string and dynamically import the model. How? <p><code>ModelMetadata.from_module</code> is an alternative way to load the model metadata. It uses importlib under the hood. In fact, our CLI uses this exact approach.</p> <pre><code>from splifft.models import ModelMetadata\n\nmy_model_metadata = ModelMetadata.from_module(\n    module_name=\"my_library.models.my_model\",\n    model_cls_name=\"MyModel\",\n    model_type=\"my_model\"\n)\n</code></pre>"},{"location":"tutorial/#3-putting-everything-together","title":"3. Putting everything together","text":"<p>First, load in the configuration:</p> <p>scripts/main.py<pre><code>from pathlib import Path\n\nfrom splifft.config import Config\n\nconfig = Config.from_file(Path(\"path/to/my_model_config.json\"))\n</code></pre> This validates your JSON and returns a pydantic.BaseModel. Note that at this point, <code>config.model</code> is a lazy model configuration that is not yet fully validated.</p> <p>Next, we need to create the PyTorch model. Concretize the lazy model configuration into the <code>dataclass</code> we defined earlier then instantiate the model: scripts/main.py<pre><code>from my_library.models import my_model_metadata\n\nmetadata = my_model_metadata()\nmy_model_params = config.model.to_concrete(metadata.params)\nmodel = metadata.model(my_model_params)\n</code></pre></p> <p>Finally, load the weights, input audio and run! scripts/main.py<pre><code>from splifft.inference import run_inference_on_file\nfrom splifft.io import load_weights, read_audio\n\ncheckpoint_path = Path(\"path/to/my_model.pt\")\nmodel = load_weights(model, checkpoint_path, device=\"cpu\")\n\nmixture = read_audio(\n    Path(\"path/to/mixture.wav\"), config.audio_io.target_sample_rate, config.audio_io.force_channels\n)\nstems = run_inference_on_file(mixture, config, model, my_model_params)\n\nprint(f\"{list(stems.keys())=}\")\n</code></pre></p>"},{"location":"api/","title":"All","text":""},{"location":"api/#splifft","title":"splifft","text":"<p>Lightweight utilities for music source separation.</p> <p>Modules:</p> Name Description <code>__main__</code> <p>Command line interface for <code>splifft</code>.</p> <code>config</code> <p>Configuration</p> <code>core</code> <p>Reusable, pure algorithmic components for inference and training.</p> <code>inference</code> <p>High level orchestrator for model inference</p> <code>io</code> <p>Operations for reading and writing to disk. All side effects should go here.</p> <code>models</code> <p>Source separation models.</p> <code>training</code> <p>High level orchestrator for model training</p> <code>types</code> <p>Types for documentation and data validation (for use in pydantic).</p> <p>Attributes:</p> Name Type Description <code>PATH_MODULE</code> <code>PATH_BASE</code> <code>PATH_DATA</code> <code>PATH_CONFIG</code> <code>PATH_MODELS</code> <code>PATH_SCRIPTS</code> <code>PATH_DOCS</code> <code>PATH_DOCS_ASSETS</code>"},{"location":"api/#splifft.PATH_MODULE","title":"PATH_MODULE  <code>module-attribute</code>","text":"<pre><code>PATH_MODULE = parent\n</code></pre>"},{"location":"api/#splifft.PATH_BASE","title":"PATH_BASE  <code>module-attribute</code>","text":"<pre><code>PATH_BASE = parent\n</code></pre>"},{"location":"api/#splifft.PATH_DATA","title":"PATH_DATA  <code>module-attribute</code>","text":"<pre><code>PATH_DATA = PATH_BASE / 'data'\n</code></pre>"},{"location":"api/#splifft.PATH_CONFIG","title":"PATH_CONFIG  <code>module-attribute</code>","text":"<pre><code>PATH_CONFIG = PATH_DATA / 'config'\n</code></pre>"},{"location":"api/#splifft.PATH_MODELS","title":"PATH_MODELS  <code>module-attribute</code>","text":"<pre><code>PATH_MODELS = PATH_DATA / 'models'\n</code></pre>"},{"location":"api/#splifft.PATH_SCRIPTS","title":"PATH_SCRIPTS  <code>module-attribute</code>","text":"<pre><code>PATH_SCRIPTS = PATH_BASE / 'scripts'\n</code></pre>"},{"location":"api/#splifft.PATH_DOCS","title":"PATH_DOCS  <code>module-attribute</code>","text":"<pre><code>PATH_DOCS = PATH_BASE / 'docs'\n</code></pre>"},{"location":"api/#splifft.PATH_DOCS_ASSETS","title":"PATH_DOCS_ASSETS  <code>module-attribute</code>","text":"<pre><code>PATH_DOCS_ASSETS = PATH_DOCS / 'assets'\n</code></pre>"},{"location":"api/config/","title":"Config","text":""},{"location":"api/config/#splifft.config","title":"config","text":"<p>Configuration</p> <p>Classes:</p> Name Description <code>LazyModelConfig</code> <p>A lazily validated model configuration.</p> <code>StftConfig</code> <p>configuration for the short-time fourier transform.</p> <code>AudioIOConfig</code> <code>TorchCompileConfig</code> <code>InferenceConfig</code> <code>ChunkingConfig</code> <code>MaskingConfig</code> <code>SubtractConfig</code> <code>SumConfig</code> <code>OutputConfig</code> <code>Config</code> <code>Model</code> <code>Metrics</code> <code>Resource</code> <code>Comment</code> <code>Registry</code> <p>Attributes:</p> Name Type Description <code>TorchDtype</code> <code>TypeAlias</code> <code>Tuple</code> <code>NonEmptyUnique</code> <code>ModelInputStemName</code> <code>TypeAlias</code> <code>ModelOutputStemName</code> <code>TypeAlias</code> <code>DerivedStemName</code> <code>TypeAlias</code> <p>The name of a derived stem, e.g. <code>vocals_minus_drums</code>.</p> <code>StemName</code> <code>TypeAlias</code> <p>A name of a stem, either a model output stem or a derived stem.</p> <code>DerivedStemRule</code> <code>TypeAlias</code> <code>DerivedStemsConfig</code> <code>TypeAlias</code>"},{"location":"api/config/#splifft.config.TorchDtype","title":"TorchDtype  <code>module-attribute</code>","text":"<pre><code>TorchDtype: TypeAlias = Annotated[\n    dtype, GetPydanticSchema(_get_torch_dtype_schema)\n]\n</code></pre>"},{"location":"api/config/#splifft.config.Tuple","title":"Tuple  <code>module-attribute</code>","text":"<pre><code>Tuple = Annotated[\n    tuple[_Item, ...], BeforeValidator(_to_tuple)\n]\n</code></pre>"},{"location":"api/config/#splifft.config.NonEmptyUnique","title":"NonEmptyUnique  <code>module-attribute</code>","text":"<pre><code>NonEmptyUnique = Annotated[\n    _S,\n    Len(min_length=1),\n    AfterValidator(_validate_unique_sequence),\n    Field(json_schema_extra={\"unique_items\": True}),\n]\n</code></pre>"},{"location":"api/config/#splifft.config.ModelInputStemName","title":"ModelInputStemName  <code>module-attribute</code>","text":"<pre><code>ModelInputStemName: TypeAlias = Literal['mixture']\n</code></pre>"},{"location":"api/config/#splifft.config.ModelOutputStemName","title":"ModelOutputStemName  <code>module-attribute</code>","text":"<pre><code>ModelOutputStemName: TypeAlias = Annotated[\n    ModelOutputStemName, StringConstraints(min_length=1)\n]\n</code></pre>"},{"location":"api/config/#splifft.config.LazyModelConfig","title":"LazyModelConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>A lazily validated model configuration.</p> <p>Note that it is not guaranteed to be fully valid until <code>to_concrete</code> is called.</p> <p>Methods:</p> Name Description <code>to_concrete</code> <p>Validate against a real set of model parameters and convert to it.</p> <p>Attributes:</p> Name Type Description <code>chunk_size</code> <code>ChunkSize</code> <code>output_stem_names</code> <code>NonEmptyUnique[Tuple[ModelOutputStemName]]</code> <code>stem_names</code> <code>tuple[ModelInputStemName | ModelOutputStemName, ...]</code> <p>Returns the model's input and output stem names.</p> <code>model_config</code>"},{"location":"api/config/#splifft.config.LazyModelConfig.chunk_size","title":"chunk_size  <code>instance-attribute</code>","text":"<pre><code>chunk_size: ChunkSize\n</code></pre>"},{"location":"api/config/#splifft.config.LazyModelConfig.output_stem_names","title":"output_stem_names  <code>instance-attribute</code>","text":"<pre><code>output_stem_names: NonEmptyUnique[\n    Tuple[ModelOutputStemName]\n]\n</code></pre>"},{"location":"api/config/#splifft.config.LazyModelConfig.to_concrete","title":"to_concrete","text":"<pre><code>to_concrete(\n    model_params: type[ModelParamsLikeT],\n    *,\n    pydantic_config: ConfigDict = ConfigDict(\n        extra=\"forbid\"\n    ),\n) -&gt; ModelParamsLikeT\n</code></pre> <p>Validate against a real set of model parameters and convert to it.</p> <p>Raises:</p> Type Description <code>pydantic.ValidationError</code> <p>if extra fields are present in the model parameters that doesn't exist in the concrete model parameters.</p> Source code in <code>src/splifft/config.py</code> <pre><code>def to_concrete(\n    self,\n    model_params: type[ModelParamsLikeT],\n    *,\n    pydantic_config: ConfigDict = ConfigDict(extra=\"forbid\"),\n) -&gt; ModelParamsLikeT:\n    \"\"\"Validate against a real set of model parameters and convert to it.\n\n    :raises pydantic.ValidationError: if extra fields are present in the model parameters\n        that doesn't exist in the concrete model parameters.\n    \"\"\"\n    # input_type and output_type are inconfigurable anyway\n    # TODO: use lru cache to avoid recreating the TypeAdapter in a hot loop but dict isn't hashable\n    ta = TypeAdapter(\n        type(\n            f\"{model_params.__name__}Validator\",\n            (model_params,),\n            {\"__pydantic_config__\": pydantic_config},\n        )  # needed for https://docs.pydantic.dev/latest/errors/usage_errors/#type-adapter-config-unused\n    )  # type: ignore\n    # types defined within `TYPE_CHECKING` blocks will be forward references, so we need rebuild\n    ta.rebuild(_types_namespace={\"TorchDtype\": TorchDtype, \"t\": t})\n    model_params_concrete: ModelParamsLikeT = ta.validate_python(self.model_dump())  # type: ignore\n    return model_params_concrete\n</code></pre>"},{"location":"api/config/#splifft.config.LazyModelConfig.stem_names","title":"stem_names  <code>property</code>","text":"<pre><code>stem_names: tuple[\n    ModelInputStemName | ModelOutputStemName, ...\n]\n</code></pre> <p>Returns the model's input and output stem names.</p>"},{"location":"api/config/#splifft.config.LazyModelConfig.model_config","title":"model_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_config = ConfigDict(strict=True, extra='allow')\n</code></pre>"},{"location":"api/config/#splifft.config.StftConfig","title":"StftConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>configuration for the short-time fourier transform.</p> <p>Attributes:</p> Name Type Description <code>n_fft</code> <code>FftSize</code> <code>hop_length</code> <code>HopSize</code> <code>win_length</code> <code>FftSize</code> <code>window_shape</code> <code>WindowShape</code> <code>normalized</code> <code>bool</code> <code>conv_dtype</code> <code>TorchDtype | None</code> <p>The data type used for the <code>conv1d</code> buffers.</p> <code>model_config</code>"},{"location":"api/config/#splifft.config.StftConfig.n_fft","title":"n_fft  <code>instance-attribute</code>","text":"<pre><code>n_fft: FftSize\n</code></pre>"},{"location":"api/config/#splifft.config.StftConfig.hop_length","title":"hop_length  <code>instance-attribute</code>","text":"<pre><code>hop_length: HopSize\n</code></pre>"},{"location":"api/config/#splifft.config.StftConfig.win_length","title":"win_length  <code>instance-attribute</code>","text":"<pre><code>win_length: FftSize\n</code></pre>"},{"location":"api/config/#splifft.config.StftConfig.window_shape","title":"window_shape  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>window_shape: WindowShape = 'hann'\n</code></pre>"},{"location":"api/config/#splifft.config.StftConfig.normalized","title":"normalized  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>normalized: bool = False\n</code></pre>"},{"location":"api/config/#splifft.config.StftConfig.conv_dtype","title":"conv_dtype  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>conv_dtype: TorchDtype | None = None\n</code></pre> <p>The data type used for the <code>conv1d</code> buffers.</p>"},{"location":"api/config/#splifft.config.StftConfig.model_config","title":"model_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_config = _PYDANTIC_STRICT_CONFIG\n</code></pre>"},{"location":"api/config/#splifft.config.AudioIOConfig","title":"AudioIOConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Attributes:</p> Name Type Description <code>target_sample_rate</code> <code>SampleRate</code> <code>force_channels</code> <code>Channels | None</code> <p>Whether to force mono or stereo audio input. If None, keep original.</p> <code>model_config</code>"},{"location":"api/config/#splifft.config.AudioIOConfig.target_sample_rate","title":"target_sample_rate  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>target_sample_rate: SampleRate = 44100\n</code></pre>"},{"location":"api/config/#splifft.config.AudioIOConfig.force_channels","title":"force_channels  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>force_channels: Channels | None = 2\n</code></pre> <p>Whether to force mono or stereo audio input. If None, keep original.</p>"},{"location":"api/config/#splifft.config.AudioIOConfig.model_config","title":"model_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_config = _PYDANTIC_STRICT_CONFIG\n</code></pre>"},{"location":"api/config/#splifft.config.TorchCompileConfig","title":"TorchCompileConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Attributes:</p> Name Type Description <code>fullgraph</code> <code>bool</code> <code>dynamic</code> <code>bool</code> <code>mode</code> <code>Literal['default', 'reduce-overhead', 'max-autotune', 'max-autotune-no-cudagraphs']</code>"},{"location":"api/config/#splifft.config.TorchCompileConfig.fullgraph","title":"fullgraph  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>fullgraph: bool = True\n</code></pre>"},{"location":"api/config/#splifft.config.TorchCompileConfig.dynamic","title":"dynamic  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>dynamic: bool = True\n</code></pre>"},{"location":"api/config/#splifft.config.TorchCompileConfig.mode","title":"mode  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mode: Literal[\n    \"default\",\n    \"reduce-overhead\",\n    \"max-autotune\",\n    \"max-autotune-no-cudagraphs\",\n] = \"reduce-overhead\"\n</code></pre>"},{"location":"api/config/#splifft.config.InferenceConfig","title":"InferenceConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Attributes:</p> Name Type Description <code>normalize_input_audio</code> <code>bool</code> <code>batch_size</code> <code>BatchSize</code> <code>force_weights_dtype</code> <code>TorchDtype | None</code> <code>use_autocast_dtype</code> <code>TorchDtype | None</code> <code>compile_model</code> <code>TorchCompileConfig | None</code> <code>apply_tta</code> <code>bool</code> <code>model_config</code>"},{"location":"api/config/#splifft.config.InferenceConfig.normalize_input_audio","title":"normalize_input_audio  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>normalize_input_audio: bool = False\n</code></pre>"},{"location":"api/config/#splifft.config.InferenceConfig.batch_size","title":"batch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>batch_size: BatchSize = 8\n</code></pre>"},{"location":"api/config/#splifft.config.InferenceConfig.force_weights_dtype","title":"force_weights_dtype  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>force_weights_dtype: TorchDtype | None = None\n</code></pre>"},{"location":"api/config/#splifft.config.InferenceConfig.use_autocast_dtype","title":"use_autocast_dtype  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>use_autocast_dtype: TorchDtype | None = None\n</code></pre>"},{"location":"api/config/#splifft.config.InferenceConfig.compile_model","title":"compile_model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>compile_model: TorchCompileConfig | None = None\n</code></pre>"},{"location":"api/config/#splifft.config.InferenceConfig.apply_tta","title":"apply_tta  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>apply_tta: bool = False\n</code></pre>"},{"location":"api/config/#splifft.config.InferenceConfig.model_config","title":"model_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_config = _PYDANTIC_STRICT_CONFIG\n</code></pre>"},{"location":"api/config/#splifft.config.ChunkingConfig","title":"ChunkingConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Attributes:</p> Name Type Description <code>method</code> <code>Literal['overlap_add_windowed']</code> <code>overlap_ratio</code> <code>OverlapRatio</code> <code>window_shape</code> <code>WindowShape</code> <code>padding_mode</code> <code>PaddingMode</code> <code>model_config</code>"},{"location":"api/config/#splifft.config.ChunkingConfig.method","title":"method  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>method: Literal[\"overlap_add_windowed\"] = (\n    \"overlap_add_windowed\"\n)\n</code></pre>"},{"location":"api/config/#splifft.config.ChunkingConfig.overlap_ratio","title":"overlap_ratio  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overlap_ratio: OverlapRatio = 0.5\n</code></pre>"},{"location":"api/config/#splifft.config.ChunkingConfig.window_shape","title":"window_shape  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>window_shape: WindowShape = 'hann'\n</code></pre>"},{"location":"api/config/#splifft.config.ChunkingConfig.padding_mode","title":"padding_mode  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>padding_mode: PaddingMode = 'reflect'\n</code></pre>"},{"location":"api/config/#splifft.config.ChunkingConfig.model_config","title":"model_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_config = _PYDANTIC_STRICT_CONFIG\n</code></pre>"},{"location":"api/config/#splifft.config.MaskingConfig","title":"MaskingConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Attributes:</p> Name Type Description <code>add_sub_dtype</code> <code>TorchDtype | None</code> <code>out_dtype</code> <code>TorchDtype | None</code> <code>model_config</code>"},{"location":"api/config/#splifft.config.MaskingConfig.add_sub_dtype","title":"add_sub_dtype  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>add_sub_dtype: TorchDtype | None = None\n</code></pre>"},{"location":"api/config/#splifft.config.MaskingConfig.out_dtype","title":"out_dtype  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>out_dtype: TorchDtype | None = None\n</code></pre>"},{"location":"api/config/#splifft.config.MaskingConfig.model_config","title":"model_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_config = _PYDANTIC_STRICT_CONFIG\n</code></pre>"},{"location":"api/config/#splifft.config.DerivedStemName","title":"DerivedStemName  <code>module-attribute</code>","text":"<pre><code>DerivedStemName: TypeAlias = Annotated[\n    str, StringConstraints(min_length=1)\n]\n</code></pre> <p>The name of a derived stem, e.g. <code>vocals_minus_drums</code>.</p>"},{"location":"api/config/#splifft.config.StemName","title":"StemName  <code>module-attribute</code>","text":"<pre><code>StemName: TypeAlias = Union[\n    ModelOutputStemName, DerivedStemName\n]\n</code></pre> <p>A name of a stem, either a model output stem or a derived stem.</p>"},{"location":"api/config/#splifft.config.SubtractConfig","title":"SubtractConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Attributes:</p> Name Type Description <code>operation</code> <code>Literal['subtract']</code> <code>stem_name</code> <code>StemName</code> <code>by_stem_name</code> <code>StemName</code> <code>model_config</code>"},{"location":"api/config/#splifft.config.SubtractConfig.operation","title":"operation  <code>instance-attribute</code>","text":"<pre><code>operation: Literal['subtract']\n</code></pre>"},{"location":"api/config/#splifft.config.SubtractConfig.stem_name","title":"stem_name  <code>instance-attribute</code>","text":"<pre><code>stem_name: StemName\n</code></pre>"},{"location":"api/config/#splifft.config.SubtractConfig.by_stem_name","title":"by_stem_name  <code>instance-attribute</code>","text":"<pre><code>by_stem_name: StemName\n</code></pre>"},{"location":"api/config/#splifft.config.SubtractConfig.model_config","title":"model_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_config = _PYDANTIC_STRICT_CONFIG\n</code></pre>"},{"location":"api/config/#splifft.config.SumConfig","title":"SumConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Attributes:</p> Name Type Description <code>operation</code> <code>Literal['sum']</code> <code>stem_names</code> <code>NonEmptyUnique[Tuple[StemName]]</code> <code>model_config</code>"},{"location":"api/config/#splifft.config.SumConfig.operation","title":"operation  <code>instance-attribute</code>","text":"<pre><code>operation: Literal['sum']\n</code></pre>"},{"location":"api/config/#splifft.config.SumConfig.stem_names","title":"stem_names  <code>instance-attribute</code>","text":"<pre><code>stem_names: NonEmptyUnique[Tuple[StemName]]\n</code></pre>"},{"location":"api/config/#splifft.config.SumConfig.model_config","title":"model_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_config = _PYDANTIC_STRICT_CONFIG\n</code></pre>"},{"location":"api/config/#splifft.config.DerivedStemRule","title":"DerivedStemRule  <code>module-attribute</code>","text":"<pre><code>DerivedStemRule: TypeAlias = Annotated[\n    Union[SubtractConfig, SumConfig],\n    Discriminator(\"operation\"),\n]\n</code></pre>"},{"location":"api/config/#splifft.config.DerivedStemsConfig","title":"DerivedStemsConfig  <code>module-attribute</code>","text":"<pre><code>DerivedStemsConfig: TypeAlias = dict[\n    DerivedStemName, DerivedStemRule\n]\n</code></pre>"},{"location":"api/config/#splifft.config.OutputConfig","title":"OutputConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Attributes:</p> Name Type Description <code>stem_names</code> <code>Literal['all'] | NonEmptyUnique[Tuple[StemName]]</code> <code>file_format</code> <code>FileFormat</code> <code>bit_rate</code> <code>BitRate | None</code> <p>Output bit rate for lossy formats. The default is chosen by FFmpeg.</p> <code>model_config</code>"},{"location":"api/config/#splifft.config.OutputConfig.stem_names","title":"stem_names  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>stem_names: (\n    Literal[\"all\"] | NonEmptyUnique[Tuple[StemName]]\n) = \"all\"\n</code></pre>"},{"location":"api/config/#splifft.config.OutputConfig.file_format","title":"file_format  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>file_format: FileFormat = 'wav'\n</code></pre>"},{"location":"api/config/#splifft.config.OutputConfig.bit_rate","title":"bit_rate  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>bit_rate: BitRate | None = None\n</code></pre> <p>Output bit rate for lossy formats. The default is chosen by FFmpeg.</p>"},{"location":"api/config/#splifft.config.OutputConfig.model_config","title":"model_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_config = _PYDANTIC_STRICT_CONFIG\n</code></pre>"},{"location":"api/config/#splifft.config.Config","title":"Config","text":"<p>               Bases: <code>BaseModel</code></p> <p>Methods:</p> Name Description <code>check_derived_stems</code> <code>from_file</code> <p>Attributes:</p> Name Type Description <code>identifier</code> <code>str</code> <p>Unique identifier for this configuration</p> <code>model_type</code> <code>ModelType</code> <code>model</code> <code>LazyModelConfig</code> <code>stft</code> <code>StftConfig | None</code> <code>audio_io</code> <code>AudioIOConfig</code> <code>inference</code> <code>InferenceConfig</code> <code>chunking</code> <code>ChunkingConfig</code> <code>masking</code> <code>MaskingConfig</code> <code>derived_stems</code> <code>DerivedStemsConfig | None</code> <code>output</code> <code>OutputConfig</code> <code>experimental</code> <code>dict[str, Any] | None</code> <p>Any extra experimental configurations outside of the <code>splifft</code> core.</p> <code>model_config</code>"},{"location":"api/config/#splifft.config.Config.identifier","title":"identifier  <code>instance-attribute</code>","text":"<pre><code>identifier: str\n</code></pre> <p>Unique identifier for this configuration</p>"},{"location":"api/config/#splifft.config.Config.model_type","title":"model_type  <code>instance-attribute</code>","text":"<pre><code>model_type: ModelType\n</code></pre>"},{"location":"api/config/#splifft.config.Config.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model: LazyModelConfig\n</code></pre>"},{"location":"api/config/#splifft.config.Config.stft","title":"stft  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>stft: StftConfig | None = None\n</code></pre>"},{"location":"api/config/#splifft.config.Config.audio_io","title":"audio_io  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>audio_io: AudioIOConfig = Field(\n    default_factory=AudioIOConfig\n)\n</code></pre>"},{"location":"api/config/#splifft.config.Config.inference","title":"inference  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>inference: InferenceConfig = Field(\n    default_factory=InferenceConfig\n)\n</code></pre>"},{"location":"api/config/#splifft.config.Config.chunking","title":"chunking  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>chunking: ChunkingConfig = Field(\n    default_factory=ChunkingConfig\n)\n</code></pre>"},{"location":"api/config/#splifft.config.Config.masking","title":"masking  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>masking: MaskingConfig = Field(\n    default_factory=MaskingConfig\n)\n</code></pre>"},{"location":"api/config/#splifft.config.Config.derived_stems","title":"derived_stems  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>derived_stems: DerivedStemsConfig | None = None\n</code></pre>"},{"location":"api/config/#splifft.config.Config.output","title":"output  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output: OutputConfig = Field(default_factory=OutputConfig)\n</code></pre>"},{"location":"api/config/#splifft.config.Config.experimental","title":"experimental  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>experimental: dict[str, Any] | None = None\n</code></pre> <p>Any extra experimental configurations outside of the <code>splifft</code> core.</p>"},{"location":"api/config/#splifft.config.Config.check_derived_stems","title":"check_derived_stems","text":"<pre><code>check_derived_stems() -&gt; Self\n</code></pre> Source code in <code>src/splifft/config.py</code> <pre><code>@model_validator(mode=\"after\")\ndef check_derived_stems(self) -&gt; Self:\n    if self.derived_stems is None:\n        return self\n    existing_stem_names: list[StemName] = list(self.model.stem_names)\n    for derived_stem_name, definition in self.derived_stems.items():\n        if derived_stem_name in existing_stem_names:\n            raise PydanticCustomError(\n                \"derived_stem_name_conflict\",\n                \"Derived stem `{derived_stem_name}` must not conflict with existing stem names: `{existing_stem_names}`\",\n                {\n                    \"derived_stem_name\": derived_stem_name,\n                    \"existing_stem_names\": existing_stem_names,\n                },\n            )\n        required_stems: tuple[StemName, ...] = tuple()\n        if isinstance(definition, SubtractConfig):\n            required_stems = (definition.stem_name, definition.by_stem_name)\n        elif isinstance(definition, SumConfig):\n            required_stems = definition.stem_names\n        for stem_name in required_stems:\n            if stem_name not in existing_stem_names:\n                raise PydanticCustomError(\n                    \"invalid_derived_stem\",\n                    \"Derived stem `{derived_stem_name}` requires stem `{stem_name}` but is not found in `{existing_stem_names}`\",\n                    {\n                        \"derived_stem_name\": derived_stem_name,\n                        \"stem_name\": stem_name,\n                        \"existing_stem_names\": existing_stem_names,\n                    },\n                )\n        existing_stem_names.append(derived_stem_name)\n    return self\n</code></pre>"},{"location":"api/config/#splifft.config.Config.from_file","title":"from_file  <code>classmethod</code>","text":"<pre><code>from_file(path: BytesPath) -&gt; Config\n</code></pre> Source code in <code>src/splifft/config.py</code> <pre><code>@classmethod\ndef from_file(cls, path: t.BytesPath) -&gt; Config:\n    with open(path, \"rb\") as f:\n        return Config.model_validate_json(f.read())\n</code></pre>"},{"location":"api/config/#splifft.config.Config.model_config","title":"model_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_config = ConfigDict(\n    arbitrary_types_allowed=True,\n    strict=True,\n    extra=\"forbid\",\n)\n</code></pre>"},{"location":"api/config/#splifft.config.Model","title":"Model","text":"<p>               Bases: <code>BaseModel</code></p> <p>Attributes:</p> Name Type Description <code>authors</code> <code>list[str]</code> <code>purpose</code> <code>Literal['separation', 'denoise', 'de-reverb', 'enhancement', 'crowd_removal', 'upscaler', 'phase_fixer'] | str</code> <code>architecture</code> <code>Literal['bs_roformer', 'mel_roformer', 'mdx23c', 'scnet'] | str</code> <code>release_date</code> <code>str | None</code> <p>YYYY-MM-DD, date is optional</p> <code>finetuned_from</code> <code>Identifier | None</code> <code>output</code> <code>list[Instrument]</code> <code>status</code> <code>Literal['alpha', 'beta', 'stable', 'deprecated'] | None</code> <code>metrics</code> <code>list[Metrics]</code> <code>description</code> <code>list[Comment]</code> <code>approx_model_size_mb</code> <code>float | None</code>"},{"location":"api/config/#splifft.config.Model.authors","title":"authors  <code>instance-attribute</code>","text":"<pre><code>authors: list[str]\n</code></pre>"},{"location":"api/config/#splifft.config.Model.purpose","title":"purpose  <code>instance-attribute</code>","text":"<pre><code>purpose: (\n    Literal[\n        \"separation\",\n        \"denoise\",\n        \"de-reverb\",\n        \"enhancement\",\n        \"crowd_removal\",\n        \"upscaler\",\n        \"phase_fixer\",\n    ]\n    | str\n)\n</code></pre>"},{"location":"api/config/#splifft.config.Model.architecture","title":"architecture  <code>instance-attribute</code>","text":"<pre><code>architecture: (\n    Literal[\n        \"bs_roformer\", \"mel_roformer\", \"mdx23c\", \"scnet\"\n    ]\n    | str\n)\n</code></pre>"},{"location":"api/config/#splifft.config.Model.release_date","title":"release_date  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>release_date: str | None = None\n</code></pre> <p>YYYY-MM-DD, date is optional</p>"},{"location":"api/config/#splifft.config.Model.finetuned_from","title":"finetuned_from  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>finetuned_from: Identifier | None = None\n</code></pre>"},{"location":"api/config/#splifft.config.Model.output","title":"output  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output: list[Instrument] = Field(default_factory=list)\n</code></pre>"},{"location":"api/config/#splifft.config.Model.status","title":"status  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>status: (\n    Literal[\"alpha\", \"beta\", \"stable\", \"deprecated\"] | None\n) = None\n</code></pre>"},{"location":"api/config/#splifft.config.Model.metrics","title":"metrics  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>metrics: list[Metrics] = Field(default_factory=list)\n</code></pre>"},{"location":"api/config/#splifft.config.Model.description","title":"description  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>description: list[Comment] = Field(default_factory=list)\n</code></pre>"},{"location":"api/config/#splifft.config.Model.approx_model_size_mb","title":"approx_model_size_mb  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>approx_model_size_mb: float | None = None\n</code></pre>"},{"location":"api/config/#splifft.config.Metrics","title":"Metrics","text":"<p>               Bases: <code>BaseModel</code></p> <p>Attributes:</p> Name Type Description <code>values</code> <code>dict[Instrument, dict[Metric, float]]</code> <code>source</code> <code>Literal['mvsep'] | str | None</code>"},{"location":"api/config/#splifft.config.Metrics.values","title":"values  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>values: dict[Instrument, dict[Metric, float]] = Field(\n    default_factory=dict\n)\n</code></pre>"},{"location":"api/config/#splifft.config.Metrics.source","title":"source  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>source: Literal['mvsep'] | str | None = None\n</code></pre>"},{"location":"api/config/#splifft.config.Resource","title":"Resource","text":"<p>               Bases: <code>BaseModel</code></p> <p>Attributes:</p> Name Type Description <code>kind</code> <code>Literal['model_ckpt', 'huggingface', 'config_msst', 'colab', 'mvsep', 'other']</code> <code>url</code> <code>str</code> <code>digest</code> <code>str | None</code>"},{"location":"api/config/#splifft.config.Resource.kind","title":"kind  <code>instance-attribute</code>","text":"<pre><code>kind: Literal[\n    \"model_ckpt\",\n    \"huggingface\",\n    \"config_msst\",\n    \"colab\",\n    \"mvsep\",\n    \"other\",\n]\n</code></pre>"},{"location":"api/config/#splifft.config.Resource.url","title":"url  <code>instance-attribute</code>","text":"<pre><code>url: str\n</code></pre>"},{"location":"api/config/#splifft.config.Resource.digest","title":"digest  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>digest: str | None = None\n</code></pre>"},{"location":"api/config/#splifft.config.Comment","title":"Comment","text":"<p>               Bases: <code>BaseModel</code></p> <p>Attributes:</p> Name Type Description <code>content</code> <code>list[str]</code> <p>Condensed informative points of the model (lowercase)</p> <code>author</code> <code>str | None</code>"},{"location":"api/config/#splifft.config.Comment.content","title":"content  <code>instance-attribute</code>","text":"<pre><code>content: list[str]\n</code></pre> <p>Condensed informative points of the model (lowercase)</p>"},{"location":"api/config/#splifft.config.Comment.author","title":"author  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>author: str | None = None\n</code></pre>"},{"location":"api/config/#splifft.config.Registry","title":"Registry","text":"<p>               Bases: <code>dict[Identifier, Model]</code></p> <p>Methods:</p> Name Description <code>__get_pydantic_core_schema__</code> <code>from_file</code>"},{"location":"api/config/#splifft.config.Registry.__get_pydantic_core_schema__","title":"__get_pydantic_core_schema__  <code>classmethod</code>","text":"<pre><code>__get_pydantic_core_schema__(\n    source_type: Any, handler: GetCoreSchemaHandler\n) -&gt; CoreSchema\n</code></pre> Source code in <code>src/splifft/config.py</code> <pre><code>@classmethod\ndef __get_pydantic_core_schema__(\n    cls, source_type: Any, handler: GetCoreSchemaHandler\n) -&gt; CoreSchema:\n    return core_schema.no_info_after_validator_function(cls, handler(dict[t.Identifier, Model]))\n</code></pre>"},{"location":"api/config/#splifft.config.Registry.from_file","title":"from_file  <code>classmethod</code>","text":"<pre><code>from_file(path: StrOrBytesPath) -&gt; Registry\n</code></pre> Source code in <code>src/splifft/config.py</code> <pre><code>@classmethod\ndef from_file(cls, path: t.StrOrBytesPath) -&gt; Registry:\n    with open(path, \"r\") as f:\n        data = f.read()\n    ta = TypeAdapter(cls)\n    return ta.validate_json(data)\n</code></pre>"},{"location":"api/core/","title":"Core","text":""},{"location":"api/core/#splifft.core","title":"core","text":"<p>Reusable, pure algorithmic components for inference and training.</p> <p>Classes:</p> Name Description <code>Audio</code> <code>NormalizationStats</code> <p>Statistics for normalizing</p> <code>NormalizedAudio</code> <p>Container for normalized audio and its original stats.</p> <code>ModelWaveformToWaveform</code> <p>Functions:</p> Name Description <code>normalize_audio</code> <p>Preprocess the raw audio in the time domain to have a mean of 0 and a std of 1</p> <code>denormalize_audio</code> <p>Take the model output and restore them to their original loudness.</p> <code>generate_chunks</code> <p>Generates batches of overlapping chunks from an audio tensor.</p> <code>stitch_chunks</code> <p>Stitches processed audio chunks back together using the overlap-add method.</p> <code>apply_mask</code> <p>Applies a complex mask to a spectrogram.</p> <code>create_w2w_model</code> <code>derive_stems</code> <p>It is the caller's responsibility to ensure that all tensors are aligned and have the same shape.</p> <code>str_to_torch_dtype</code>"},{"location":"api/core/#splifft.core.Audio","title":"Audio  <code>dataclass</code>","text":"<pre><code>Audio(data: _AudioTensorLike, sample_rate: SampleRate)\n</code></pre> <p>               Bases: <code>Generic[_AudioTensorLike]</code></p> <p>Attributes:</p> Name Type Description <code>data</code> <code>_AudioTensorLike</code> <p>This should either be an raw or a</p> <code>sample_rate</code> <code>SampleRate</code>"},{"location":"api/core/#splifft.core.Audio.data","title":"data  <code>instance-attribute</code>","text":"<pre><code>data: _AudioTensorLike\n</code></pre> <p>This should either be an raw or a normalized audio tensor.</p>"},{"location":"api/core/#splifft.core.Audio.sample_rate","title":"sample_rate  <code>instance-attribute</code>","text":"<pre><code>sample_rate: SampleRate\n</code></pre>"},{"location":"api/core/#splifft.core.NormalizationStats","title":"NormalizationStats  <code>dataclass</code>","text":"<pre><code>NormalizationStats(\n    mean: float, std: Annotated[float, Gt(0)]\n)\n</code></pre> <p>Statistics for normalizing and denormalizing audio.</p> <p>Attributes:</p> Name Type Description <code>mean</code> <code>float</code> <p>Mean \\(\\mu\\) of the mixture</p> <code>std</code> <code>Annotated[float, Gt(0)]</code> <p>Standard deviation \\(\\sigma\\) of the mixture</p>"},{"location":"api/core/#splifft.core.NormalizationStats.mean","title":"mean  <code>instance-attribute</code>","text":"<pre><code>mean: float\n</code></pre> <p>Mean \\(\\mu\\) of the mixture</p>"},{"location":"api/core/#splifft.core.NormalizationStats.std","title":"std  <code>instance-attribute</code>","text":"<pre><code>std: Annotated[float, Gt(0)]\n</code></pre> <p>Standard deviation \\(\\sigma\\) of the mixture</p>"},{"location":"api/core/#splifft.core.NormalizedAudio","title":"NormalizedAudio  <code>dataclass</code>","text":"<pre><code>NormalizedAudio(\n    audio: Audio[NormalizedAudioTensor],\n    stats: NormalizationStats,\n)\n</code></pre> <p>Container for normalized audio and its original stats.</p> <p>Attributes:</p> Name Type Description <code>audio</code> <code>Audio[NormalizedAudioTensor]</code> <code>stats</code> <code>NormalizationStats</code>"},{"location":"api/core/#splifft.core.NormalizedAudio.audio","title":"audio  <code>instance-attribute</code>","text":"<pre><code>audio: Audio[NormalizedAudioTensor]\n</code></pre>"},{"location":"api/core/#splifft.core.NormalizedAudio.stats","title":"stats  <code>instance-attribute</code>","text":"<pre><code>stats: NormalizationStats\n</code></pre>"},{"location":"api/core/#splifft.core.normalize_audio","title":"normalize_audio","text":"<pre><code>normalize_audio(\n    audio: Audio[RawAudioTensor],\n) -&gt; NormalizedAudio\n</code></pre> <p>Preprocess the raw audio in the time domain to have a mean of 0 and a std of 1 before passing it to the model.</p> <p>Operates on the mean of the channels.</p> Source code in <code>src/splifft/core.py</code> <pre><code>def normalize_audio(audio: Audio[t.RawAudioTensor]) -&gt; NormalizedAudio:\n    \"\"\"Preprocess the raw audio in the time domain to have a mean of 0 and a std of 1\n    before passing it to the model.\n\n    Operates on the mean of the [channels][splifft.types.Channels].\n    \"\"\"\n    mono_audio = audio.data.mean(dim=0)\n    mean = float(mono_audio.mean())\n    std = float(mono_audio.std())\n\n    if std &lt;= 1e-8:  # silent audio\n        return NormalizedAudio(\n            audio=Audio(data=t.NormalizedAudioTensor(audio.data), sample_rate=audio.sample_rate),\n            stats=NormalizationStats(mean, 1.0),\n        )\n\n    normalized_data = (audio.data - mean) / std\n    return NormalizedAudio(\n        audio=Audio(data=t.NormalizedAudioTensor(normalized_data), sample_rate=audio.sample_rate),\n        stats=NormalizationStats(mean, std),\n    )\n</code></pre>"},{"location":"api/core/#splifft.core.denormalize_audio","title":"denormalize_audio","text":"<pre><code>denormalize_audio(\n    audio_data: NormalizedAudioTensor,\n    stats: NormalizationStats,\n) -&gt; RawAudioTensor\n</code></pre> <p>Take the model output and restore them to their original loudness.</p> Source code in <code>src/splifft/core.py</code> <pre><code>def denormalize_audio(\n    audio_data: t.NormalizedAudioTensor, stats: NormalizationStats\n) -&gt; t.RawAudioTensor:\n    \"\"\"Take the model output and restore them to their original loudness.\"\"\"\n    return t.RawAudioTensor((audio_data * stats.std) + stats.mean)\n</code></pre>"},{"location":"api/core/#splifft.core.generate_chunks","title":"generate_chunks","text":"<pre><code>generate_chunks(\n    audio_data: RawAudioTensor | NormalizedAudioTensor,\n    chunk_size: ChunkSize,\n    hop_size: HopSize,\n    batch_size: BatchSize,\n    *,\n    padding_mode: PaddingMode = \"reflect\",\n) -&gt; Iterator[PaddedChunkedAudioTensor]\n</code></pre> <p>Generates batches of overlapping chunks from an audio tensor.</p> <p>Returns:</p> Type Description <code>Iterator[PaddedChunkedAudioTensor]</code> <p>An iterator that yields batches of chunks of shape (B, C, chunk_T).</p> Source code in <code>src/splifft/core.py</code> <pre><code>def generate_chunks(\n    audio_data: t.RawAudioTensor | t.NormalizedAudioTensor,\n    chunk_size: t.ChunkSize,\n    hop_size: t.HopSize,\n    batch_size: t.BatchSize,\n    *,\n    padding_mode: t.PaddingMode = \"reflect\",\n) -&gt; Iterator[t.PaddedChunkedAudioTensor]:\n    \"\"\"Generates batches of overlapping chunks from an audio tensor.\n\n    :return: An iterator that yields batches of chunks of shape (B, C, chunk_T).\n    \"\"\"\n    padding = chunk_size - hop_size\n    padded_audio = F.pad(audio_data, (padding, padding), mode=padding_mode)\n\n    padded_len = padded_audio.shape[-1]\n    rem = (padded_len - chunk_size) % hop_size\n    if rem != 0:\n        final_pad = hop_size - rem\n        padded_audio = F.pad(padded_audio, (0, final_pad), mode=\"constant\", value=0)\n\n    unfolded = padded_audio.unfold(\n        dimension=-1, size=chunk_size, step=hop_size\n    )  # (C, num_chunks, chunk_size)\n\n    num_chunks = unfolded.shape[1]\n    unfolded = unfolded.permute(1, 0, 2)  # (num_chunks, C, chunk_size)\n\n    for i in range(0, num_chunks, batch_size):\n        yield t.PaddedChunkedAudioTensor(unfolded[i : i + batch_size])\n</code></pre>"},{"location":"api/core/#splifft.core.stitch_chunks","title":"stitch_chunks","text":"<pre><code>stitch_chunks(\n    processed_chunks: Sequence[SeparatedChunkedTensor],\n    num_stems: NumModelStems,\n    chunk_size: ChunkSize,\n    hop_size: HopSize,\n    target_num_samples: Samples,\n    *,\n    window: WindowTensor,\n) -&gt; RawSeparatedTensor\n</code></pre> <p>Stitches processed audio chunks back together using the overlap-add method.</p> <p>Reconstructs the full audio signal from a sequence of overlapping, processed chunks. Ensures that the sum of all overlapping windows is constant at every time step: \\(\\sum_{m=-\\infty}^{\\infty} w[n - mH] = C\\) where \\(H\\) is the hop size.</p> Source code in <code>src/splifft/core.py</code> <pre><code>def stitch_chunks(\n    processed_chunks: Sequence[t.SeparatedChunkedTensor],\n    num_stems: t.NumModelStems,\n    chunk_size: t.ChunkSize,\n    hop_size: t.HopSize,\n    target_num_samples: t.Samples,\n    *,\n    window: t.WindowTensor,\n) -&gt; t.RawSeparatedTensor:\n    r\"\"\"Stitches processed audio chunks back together using the [overlap-add method](https://en.wikipedia.org/wiki/Overlap%E2%80%93add_method).\n\n    Reconstructs the full audio signal from a sequence of overlapping, processed chunks. Ensures\n    that the sum of all overlapping windows is constant at every time step:\n    $\\sum_{m=-\\infty}^{\\infty} w[n - mH] = C$ where $H$ is the [hop size][splifft.types.HopSize].\n    \"\"\"\n    all_chunks = torch.cat(tuple(processed_chunks), dim=0)\n    total_chunks, _N, num_channels, _chunk_T = all_chunks.shape\n    windowed_chunks = all_chunks * window.view(1, 1, 1, -1)\n\n    # folding: (B, N * C * chunk_T) -&gt; (1, N * C * chunk_T, total_chunks)\n    reshaped_for_fold = windowed_chunks.permute(1, 2, 3, 0).reshape(\n        1, num_stems * num_channels * chunk_size, total_chunks\n    )\n\n    total_length = (total_chunks - 1) * hop_size + chunk_size\n\n    folded = F.fold(\n        reshaped_for_fold,\n        output_size=(1, total_length),\n        kernel_size=(1, chunk_size),\n        stride=(1, hop_size),\n    )  # (1, N * C, 1, total_length)\n    stitched = folded.view(num_stems, num_channels, total_length)\n\n    # normalization for overlap-add\n    windows_to_fold = window.expand(total_chunks, 1, chunk_size)\n    reshaped_windows_for_fold = windows_to_fold.permute(1, 2, 0).reshape(\n        1, chunk_size, total_chunks\n    )\n    norm_window = F.fold(\n        reshaped_windows_for_fold,\n        output_size=(1, total_length),\n        kernel_size=(1, chunk_size),\n        stride=(1, hop_size),\n    ).squeeze(0)\n\n    norm_window.clamp_min_(1e-8)  # for edges where the window sum might be zero\n    stitched /= norm_window\n\n    padding = chunk_size - hop_size\n    if padding &gt; 0:\n        stitched = stitched[..., padding:-padding]\n\n    return t.RawSeparatedTensor(stitched[..., :target_num_samples])\n</code></pre>"},{"location":"api/core/#splifft.core.apply_mask","title":"apply_mask","text":"<pre><code>apply_mask(\n    spec_for_masking: ComplexSpectrogram,\n    mask_batch: ComplexSpectrogram,\n    mask_add_sub_dtype: dtype | None,\n    mask_out_dtype: dtype | None,\n) -&gt; SeparatedSpectrogramTensor\n</code></pre> <p>Applies a complex mask to a spectrogram.</p> <p>While this can be simply replaced by a complex multiplication and <code>torch.view_as_complex</code>, CoreML does not support it: https://github.com/apple/coremltools/issues/2003 so we handroll our own.</p> Source code in <code>src/splifft/core.py</code> <pre><code>def apply_mask(\n    spec_for_masking: t.ComplexSpectrogram,\n    mask_batch: t.ComplexSpectrogram,\n    mask_add_sub_dtype: torch.dtype | None,\n    mask_out_dtype: torch.dtype | None,\n) -&gt; t.SeparatedSpectrogramTensor:\n    \"\"\"Applies a complex mask to a spectrogram.\n\n    While this can be simply replaced by a complex multiplication and `torch.view_as_complex`,\n    CoreML does not support it: https://github.com/apple/coremltools/issues/2003 so we handroll our\n    own.\n    \"\"\"\n    spec_real = spec_for_masking[..., 0]\n    spec_imag = spec_for_masking[..., 1]\n    mask_real = mask_batch[..., 0]\n    mask_imag = mask_batch[..., 1]\n\n    # see: 14385, 14401, 14392, 14408\n    ac = spec_real * mask_real\n    bd = spec_imag * mask_imag\n    ad = spec_real * mask_imag\n    bc = spec_imag * mask_real\n\n    # see: 509, 506, 505, 504, 741, 747\n    out_real = ac.to(mask_add_sub_dtype) - bd.to(mask_add_sub_dtype)\n    out_imag = ad.to(mask_add_sub_dtype) + bc.to(mask_add_sub_dtype)\n\n    # see: 503, 501\n    separated_spec = torch.stack([out_real, out_imag], dim=-1).to(mask_out_dtype)\n    return t.SeparatedSpectrogramTensor(separated_spec)\n</code></pre>"},{"location":"api/core/#splifft.core.ModelWaveformToWaveform","title":"ModelWaveformToWaveform","text":"<pre><code>ModelWaveformToWaveform(\n    model: Module,\n    preprocess: PreprocessFn,\n    postprocess: PostprocessFn,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>model</code> <code>preprocess</code> <code>postprocess</code> Source code in <code>src/splifft/core.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    preprocess: t.PreprocessFn,\n    postprocess: t.PostprocessFn,\n):\n    super().__init__()\n    self.model = model\n    self.preprocess = preprocess\n    self.postprocess = postprocess\n</code></pre>"},{"location":"api/core/#splifft.core.ModelWaveformToWaveform.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model = model\n</code></pre>"},{"location":"api/core/#splifft.core.ModelWaveformToWaveform.preprocess","title":"preprocess  <code>instance-attribute</code>","text":"<pre><code>preprocess = preprocess\n</code></pre>"},{"location":"api/core/#splifft.core.ModelWaveformToWaveform.postprocess","title":"postprocess  <code>instance-attribute</code>","text":"<pre><code>postprocess = postprocess\n</code></pre>"},{"location":"api/core/#splifft.core.ModelWaveformToWaveform.forward","title":"forward","text":"<pre><code>forward(\n    waveform_chunk: RawAudioTensor | NormalizedAudioTensor,\n) -&gt; SeparatedChunkedTensor\n</code></pre> Source code in <code>src/splifft/core.py</code> <pre><code>def forward(\n    self, waveform_chunk: t.RawAudioTensor | t.NormalizedAudioTensor\n) -&gt; t.SeparatedChunkedTensor:\n    preprocessed_input = self.preprocess(waveform_chunk)\n    model_output = self.model(*preprocessed_input)\n    return t.SeparatedChunkedTensor(self.postprocess(model_output, *preprocessed_input))\n</code></pre>"},{"location":"api/core/#splifft.core.create_w2w_model","title":"create_w2w_model","text":"<pre><code>create_w2w_model(\n    model: Module,\n    model_input_type: ModelInputType,\n    model_output_type: ModelOutputType,\n    stft_cfg: StftConfig | None,\n    num_channels: Channels,\n    chunk_size: ChunkSize,\n    masking_cfg: MaskingConfig,\n) -&gt; ModelWaveformToWaveform\n</code></pre> Source code in <code>src/splifft/core.py</code> <pre><code>def create_w2w_model(\n    model: nn.Module,\n    model_input_type: t.ModelInputType,\n    model_output_type: t.ModelOutputType,\n    stft_cfg: StftConfig | None,\n    num_channels: t.Channels,\n    chunk_size: t.ChunkSize,\n    masking_cfg: MaskingConfig,\n) -&gt; ModelWaveformToWaveform:\n    try:\n        device = next(model.parameters()).device\n    except StopIteration:\n        device = torch.device(\"cpu\")\n\n    needs_stft = model_input_type == \"spectrogram\" or model_input_type == \"waveform_and_spectrogram\"\n    needs_istft = model_output_type == \"spectrogram_mask\" or model_output_type == \"spectrogram\"\n\n    if (needs_stft or needs_istft) and stft_cfg is None:\n        raise ValueError(\n            \"expected stft config for models that operate on spectrograms, but found `None`.\"\n        )\n\n    preprocess: t.PreprocessFn = lambda chunk: (chunk,)  # noqa: E731\n    postprocess: t.PostprocessFn = lambda model_output, *_: model_output  # noqa: E731\n\n    if needs_stft:\n        assert stft_cfg is not None\n        stft_module = Stft(\n            n_fft=stft_cfg.n_fft,\n            hop_length=stft_cfg.hop_length,\n            win_length=stft_cfg.win_length,\n            window_fn=lambda win_len: _get_window_fn(stft_cfg.window_shape, win_len, device),\n            conv_dtype=stft_cfg.conv_dtype,\n        ).to(device)\n        if model_input_type == \"spectrogram\":\n            preprocess = _create_stft_preprocessor(stft_module)\n        elif model_input_type == \"waveform_and_spectrogram\":\n            preprocess = _create_hybrid_preprocessor(stft_module)\n        else:\n            raise NotImplementedError(f\"unsupported input type for stft: {model_input_type}\")\n\n    if needs_istft:\n        assert stft_cfg is not None\n        istft_module = IStft(\n            n_fft=stft_cfg.n_fft,\n            hop_length=stft_cfg.hop_length,\n            win_length=stft_cfg.win_length,\n            window_fn=lambda win_len: _get_window_fn(stft_cfg.window_shape, win_len, device),\n        ).to(device)\n        postprocess = _create_spec_postprocessor(\n            istft_module,\n            num_channels,\n            chunk_size,\n            masking_cfg.add_sub_dtype,\n            masking_cfg.out_dtype,\n            model_output_type,\n        )\n    return ModelWaveformToWaveform(model, preprocess, postprocess)\n</code></pre>"},{"location":"api/core/#splifft.core.derive_stems","title":"derive_stems","text":"<pre><code>derive_stems(\n    separated_stems: Mapping[\n        ModelOutputStemName, RawAudioTensor\n    ],\n    mixture_input: RawAudioTensor,\n    stem_rules: DerivedStemsConfig,\n) -&gt; dict[StemName, RawAudioTensor]\n</code></pre> <p>It is the caller's responsibility to ensure that all tensors are aligned and have the same shape.</p> <p>Note</p> <p>Mixture input and separated stems must first be denormalized.</p> Source code in <code>src/splifft/core.py</code> <pre><code>def derive_stems(\n    separated_stems: Mapping[t.ModelOutputStemName, t.RawAudioTensor],\n    mixture_input: t.RawAudioTensor,\n    stem_rules: DerivedStemsConfig,\n) -&gt; dict[StemName, t.RawAudioTensor]:\n    \"\"\"\n    It is the caller's responsibility to ensure that all tensors are aligned and have the same shape.\n\n    !!! note\n        Mixture input and separated stems must first be [denormalized][splifft.core.denormalize_audio].\n    \"\"\"\n    stems = {\n        \"mixture\": t.RawAudioTensor(mixture_input),  # for subtraction\n        **separated_stems,\n    }\n\n    for derived_name, rule in stem_rules.items():\n        if rule.operation == \"subtract\":\n            minuend = stems.get(rule.stem_name, mixture_input)\n            subtrahend = stems.get(rule.by_stem_name, mixture_input)\n            stems[derived_name] = t.RawAudioTensor(minuend - subtrahend)\n        elif rule.operation == \"sum\":\n            to_sum = tuple(stems[s] for s in rule.stem_names)\n            stems[derived_name] = t.RawAudioTensor(torch.stack(to_sum).sum(dim=0))\n\n    stems.pop(\"mixture\", None)\n    return stems\n</code></pre>"},{"location":"api/core/#splifft.core.str_to_torch_dtype","title":"str_to_torch_dtype","text":"<pre><code>str_to_torch_dtype(value: Any) -&gt; dtype\n</code></pre> Source code in <code>src/splifft/core.py</code> <pre><code>def str_to_torch_dtype(value: Any) -&gt; torch.dtype:\n    if not isinstance(value, str):\n        raise TypeError(f\"expected dtype to be a string, got {value} (type {type(value)})\")\n    try:\n        dtype = getattr(torch, value)\n    except AttributeError:\n        raise ValueError(f\"`{value}` cannot be found under the `torch` namespace\")\n    if not isinstance(dtype, torch.dtype):\n        raise TypeError(f\"expected {dtype} to be a dtype but it is a {type(dtype)}\")\n    return dtype\n</code></pre>"},{"location":"api/inference/","title":"Inference","text":""},{"location":"api/inference/#splifft.inference","title":"inference","text":"<p>High level orchestrator for model inference</p> <p>Functions:</p> Name Description <code>run_inference_on_file</code> <p>Runs the full source separation pipeline on a single audio file.</p> <code>separate</code> <p>Chunk, predict and stitch.</p>"},{"location":"api/inference/#splifft.inference.run_inference_on_file","title":"run_inference_on_file","text":"<pre><code>run_inference_on_file(\n    mixture: Audio[RawAudioTensor],\n    config: Config,\n    model: Module,\n    model_params_concrete: ModelParamsLike,\n) -&gt; dict[StemName, RawAudioTensor]\n</code></pre> <p>Runs the full source separation pipeline on a single audio file.</p> Source code in <code>src/splifft/inference.py</code> <pre><code>def run_inference_on_file(\n    mixture: Audio[t.RawAudioTensor],\n    config: Config,\n    model: nn.Module,\n    model_params_concrete: ModelParamsLike,\n) -&gt; dict[StemName, t.RawAudioTensor]:\n    \"\"\"Runs the full source separation pipeline on a single audio file.\"\"\"\n    mixture_data: t.RawAudioTensor | t.NormalizedAudioTensor = mixture.data\n    mixture_stats: NormalizationStats | None = None\n    if config.inference.normalize_input_audio:\n        norm_audio = normalize_audio(mixture)\n        mixture_data = norm_audio.audio.data\n        mixture_stats = norm_audio.stats\n\n    separated_data = separate(\n        mixture_data=mixture_data,\n        chunk_cfg=config.chunking,\n        model=model,\n        batch_size=config.inference.batch_size,\n        num_model_stems=len(config.model.output_stem_names),\n        chunk_size=config.model.chunk_size,\n        model_input_type=model_params_concrete.input_type,\n        model_output_type=model_params_concrete.output_type,\n        stft_cfg=config.stft,\n        masking_cfg=config.masking,\n        use_autocast_dtype=config.inference.use_autocast_dtype,\n    )\n\n    denormalized_stems: dict[t.ModelOutputStemName, t.RawAudioTensor] = {}\n    for i, stem_name in enumerate(config.model.output_stem_names):\n        stem_data = separated_data[i, ...]\n        if mixture_stats is not None:\n            stem_data = denormalize_audio(\n                audio_data=t.NormalizedAudioTensor(stem_data),\n                stats=mixture_stats,\n            )\n        denormalized_stems[stem_name] = t.RawAudioTensor(stem_data)\n\n    if config.inference.apply_tta:\n        raise NotImplementedError\n\n    output_stems = denormalized_stems\n    if config.derived_stems:\n        output_stems = derive_stems(\n            denormalized_stems,\n            mixture.data,\n            config.derived_stems,\n        )\n\n    return output_stems\n</code></pre>"},{"location":"api/inference/#splifft.inference.separate","title":"separate","text":"<pre><code>separate(\n    mixture_data: RawAudioTensor | NormalizedAudioTensor,\n    chunk_cfg: ChunkingConfig,\n    model: Module,\n    batch_size: BatchSize,\n    num_model_stems: NumModelStems,\n    chunk_size: ChunkSize,\n    model_input_type: ModelInputType,\n    model_output_type: ModelOutputType,\n    stft_cfg: StftConfig | None,\n    masking_cfg: MaskingConfig,\n    *,\n    use_autocast_dtype: dtype | None = None,\n) -&gt; RawSeparatedTensor\n</code></pre> <p>Chunk, predict and stitch.</p> Source code in <code>src/splifft/inference.py</code> <pre><code>def separate(\n    mixture_data: t.RawAudioTensor | t.NormalizedAudioTensor,\n    chunk_cfg: ChunkingConfig,\n    model: nn.Module,\n    batch_size: t.BatchSize,\n    num_model_stems: t.NumModelStems,\n    chunk_size: t.ChunkSize,\n    model_input_type: t.ModelInputType,\n    model_output_type: t.ModelOutputType,\n    stft_cfg: StftConfig | None,\n    masking_cfg: MaskingConfig,\n    *,\n    use_autocast_dtype: torch.dtype | None = None,\n) -&gt; t.RawSeparatedTensor:\n    \"\"\"Chunk, predict and stitch.\"\"\"\n    device = mixture_data.device\n    original_num_samples = mixture_data.shape[-1]\n    hop_size = int(chunk_size * (1 - chunk_cfg.overlap_ratio))\n\n    window = _get_window_fn(chunk_cfg.window_shape, chunk_size, device)\n\n    padded_length = original_num_samples + 2 * (chunk_size - hop_size)\n    num_chunks = max(0, (padded_length - chunk_size) // hop_size + 1)\n    total_batches = math.ceil(num_chunks / batch_size)\n\n    chunk_generator = generate_chunks(\n        audio_data=mixture_data,\n        chunk_size=chunk_size,\n        hop_size=hop_size,\n        batch_size=batch_size,\n        padding_mode=chunk_cfg.padding_mode,\n    )\n\n    model_w2w = create_w2w_model(\n        model=model,\n        model_input_type=model_input_type,\n        model_output_type=model_output_type,\n        stft_cfg=stft_cfg,\n        num_channels=mixture_data.shape[0],\n        chunk_size=chunk_size,\n        masking_cfg=masking_cfg,\n    )\n\n    processed_chunks = []\n\n    dtype_str = f\" \u2022 {use_autocast_dtype}\" if use_autocast_dtype else \"\"\n    info_text = f\"[cyan](bs=[bold]{batch_size}[/bold] \u2022 {device.type}{dtype_str})[/cyan]\"\n\n    progress_columns = (\n        SpinnerColumn(),\n        TextColumn(\"[progress.description]{task.description}\"),\n        BarColumn(),\n        TaskProgressColumn(),\n        TimeRemainingColumn(),\n        TextColumn(info_text),\n    )\n\n    with Progress(*progress_columns, transient=True) as progress:\n        task = progress.add_task(\"processing chunks...\", total=total_batches)\n\n        with (\n            torch.inference_mode(),\n            torch.autocast(\n                device_type=device.type,\n                enabled=use_autocast_dtype is not None,\n                dtype=use_autocast_dtype,\n            ),\n        ):\n            for chunk_batch in chunk_generator:\n                separated_batch = model_w2w(chunk_batch)\n                processed_chunks.append(separated_batch)\n                progress.update(task, advance=1)\n\n    return stitch_chunks(\n        processed_chunks=processed_chunks,\n        num_stems=num_model_stems,\n        chunk_size=chunk_size,\n        hop_size=hop_size,\n        target_num_samples=original_num_samples,\n        window=t.WindowTensor(window),\n    )\n</code></pre>"},{"location":"api/io/","title":"IO","text":""},{"location":"api/io/#splifft.io","title":"io","text":"<p>Operations for reading and writing to disk. All side effects should go here.</p> <p>Functions:</p> Name Description <code>read_audio</code> <p>Loads, resamples and converts channels.</p> <code>load_weights</code> <p>Load the weights from a checkpoint into the given model.</p>"},{"location":"api/io/#splifft.io.read_audio","title":"read_audio","text":"<pre><code>read_audio(\n    file: StrPath,\n    target_sr: SampleRate,\n    target_channels: int | None,\n    device: device | None = None,\n) -&gt; Audio[RawAudioTensor]\n</code></pre> <p>Loads, resamples and converts channels.</p> Source code in <code>src/splifft/io.py</code> <pre><code>def read_audio(\n    file: t.StrPath,\n    target_sr: t.SampleRate,\n    target_channels: int | None,\n    device: torch.device | None = None,\n) -&gt; Audio[t.RawAudioTensor]:\n    \"\"\"Loads, resamples and converts channels.\"\"\"\n    decoder = AudioDecoder(source=file, sample_rate=target_sr, num_channels=target_channels)\n    samples = decoder.get_all_samples()\n    waveform = samples.data.to(device)\n\n    return Audio(t.RawAudioTensor(waveform), samples.sample_rate)\n</code></pre>"},{"location":"api/io/#splifft.io.load_weights","title":"load_weights","text":"<pre><code>load_weights(\n    model: ModelT,\n    checkpoint_file: StrPath | bytes,\n    device: device | str,\n    *,\n    strict: bool = False,\n) -&gt; ModelT\n</code></pre> <p>Load the weights from a checkpoint into the given model.</p> Source code in <code>src/splifft/io.py</code> <pre><code>def load_weights(\n    model: ModelT,\n    checkpoint_file: t.StrPath | bytes,\n    device: torch.device | str,\n    *,\n    strict: bool = False,\n) -&gt; ModelT:\n    \"\"\"Load the weights from a checkpoint into the given model.\"\"\"\n\n    state_dict = torch.load(checkpoint_file, map_location=device, weights_only=True)\n\n    # TODO: DataParallel and `module.` prefix\n    model.load_state_dict(state_dict, strict=strict)\n    # NOTE: do not torch.compile here!\n\n    return model.to(device)\n</code></pre>"},{"location":"api/models/","title":"Models","text":""},{"location":"api/models/#splifft.models","title":"models","text":"<p>Source separation models.</p> <p>Modules:</p> Name Description <code>bs_roformer</code> <p>Band-Split RoPE Transformer</p> <code>utils</code> <p>Classes:</p> Name Description <code>ModelParamsLike</code> <p>A trait that must be implemented to be considered a model parameter.</p> <code>ModelMetadata</code> <p>Metadata about a model, including its type, parameter class, and model class.</p> <p>Attributes:</p> Name Type Description <code>ModelT</code> <code>ModelParamsLikeT</code>"},{"location":"api/models/#splifft.models.ModelParamsLike","title":"ModelParamsLike","text":"<p>               Bases: <code>Protocol</code></p> <p>A trait that must be implemented to be considered a model parameter. Note that <code>input_type</code> and <code>output_type</code> belong to a model's definition and does not allow modification via the configuration dictionary.</p> <p>Attributes:</p> Name Type Description <code>chunk_size</code> <code>ChunkSize</code> <code>output_stem_names</code> <code>tuple[ModelOutputStemName, ...]</code> <code>input_type</code> <code>ModelInputType</code> <code>output_type</code> <code>ModelOutputType</code>"},{"location":"api/models/#splifft.models.ModelParamsLike.chunk_size","title":"chunk_size  <code>instance-attribute</code>","text":"<pre><code>chunk_size: ChunkSize\n</code></pre>"},{"location":"api/models/#splifft.models.ModelParamsLike.output_stem_names","title":"output_stem_names  <code>instance-attribute</code>","text":"<pre><code>output_stem_names: tuple[ModelOutputStemName, ...]\n</code></pre>"},{"location":"api/models/#splifft.models.ModelParamsLike.input_type","title":"input_type  <code>property</code>","text":"<pre><code>input_type: ModelInputType\n</code></pre>"},{"location":"api/models/#splifft.models.ModelParamsLike.output_type","title":"output_type  <code>property</code>","text":"<pre><code>output_type: ModelOutputType\n</code></pre>"},{"location":"api/models/#splifft.models.ModelT","title":"ModelT  <code>module-attribute</code>","text":"<pre><code>ModelT = TypeVar('ModelT', bound=Module)\n</code></pre>"},{"location":"api/models/#splifft.models.ModelParamsLikeT","title":"ModelParamsLikeT  <code>module-attribute</code>","text":"<pre><code>ModelParamsLikeT = TypeVar(\n    \"ModelParamsLikeT\", bound=ModelParamsLike\n)\n</code></pre>"},{"location":"api/models/#splifft.models.ModelMetadata","title":"ModelMetadata  <code>dataclass</code>","text":"<pre><code>ModelMetadata(\n    model_type: ModelType,\n    params: type[ModelParamsLikeT],\n    model: type[ModelT],\n)\n</code></pre> <p>               Bases: <code>Generic[ModelT, ModelParamsLikeT]</code></p> <p>Metadata about a model, including its type, parameter class, and model class.</p> <p>Methods:</p> Name Description <code>from_module</code> <p>Dynamically import a model named <code>X</code> and its parameter dataclass <code>XParams</code> under a</p> <p>Attributes:</p> Name Type Description <code>model_type</code> <code>ModelType</code> <code>params</code> <code>type[ModelParamsLikeT]</code> <code>model</code> <code>type[ModelT]</code>"},{"location":"api/models/#splifft.models.ModelMetadata.model_type","title":"model_type  <code>instance-attribute</code>","text":"<pre><code>model_type: ModelType\n</code></pre>"},{"location":"api/models/#splifft.models.ModelMetadata.params","title":"params  <code>instance-attribute</code>","text":"<pre><code>params: type[ModelParamsLikeT]\n</code></pre>"},{"location":"api/models/#splifft.models.ModelMetadata.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model: type[ModelT]\n</code></pre>"},{"location":"api/models/#splifft.models.ModelMetadata.from_module","title":"from_module  <code>classmethod</code>","text":"<pre><code>from_module(\n    module_name: str,\n    model_cls_name: str,\n    *,\n    model_type: ModelType,\n    package: str | None = None,\n) -&gt; ModelMetadata[Module, ModelParamsLike]\n</code></pre> <p>Dynamically import a model named <code>X</code> and its parameter dataclass <code>XParams</code> under a given module name (e.g. <code>splifft.models.bs_roformer</code>).</p> <p>Parameters:</p> Name Type Description Default <code>model_cls_name</code> <code>str</code> <p>The name of the model class to import, e.g. <code>BSRoformer</code>.</p> required <code>module_name</code> <code>str</code> <p>The name of the module to import, e.g. <code>splifft.models.bs_roformer</code>.</p> required <code>model_type</code> <code>ModelType</code> <p>The type of the model, e.g. <code>bs_roformer</code>.</p> required <code>package</code> <code>str | None</code> <p>The package to use as the anchor point from which to resolve the relative import. to an absolute import. This is only required when performing a relative import.</p> <code>None</code> Source code in <code>src/splifft/models/__init__.py</code> <pre><code>@classmethod\ndef from_module(\n    cls,\n    module_name: str,\n    model_cls_name: str,\n    *,\n    model_type: t.ModelType,\n    package: str | None = None,\n) -&gt; ModelMetadata[nn.Module, ModelParamsLike]:\n    \"\"\"\n    Dynamically import a model named `X` and its parameter dataclass `XParams` under a\n    given module name (e.g. `splifft.models.bs_roformer`).\n\n    :param model_cls_name: The name of the model class to import, e.g. `BSRoformer`.\n    :param module_name: The name of the module to import, e.g. `splifft.models.bs_roformer`.\n    :param model_type: The type of the model, e.g. `bs_roformer`.\n    :param package: The package to use as the anchor point from which to resolve the relative import.\n    to an absolute import. This is only required when performing a relative import.\n    \"\"\"\n    _loc = f\"{module_name=} under {package=}\"\n    try:\n        module = importlib.import_module(module_name, package)\n    except ImportError as e:\n        raise ValueError(f\"failed to find or import module for {_loc}\") from e\n\n    params_cls_name = f\"{model_cls_name}Params\"\n    model_cls = getattr(module, model_cls_name, None)\n    params_cls = getattr(module, params_cls_name, None)\n    if model_cls is None or params_cls is None:\n        raise AttributeError(\n            f\"expected to find a class named `{params_cls_name}` in {_loc}, but it was not found.\"\n        )\n\n    return ModelMetadata(\n        model_type=model_type,\n        model=model_cls,\n        params=params_cls,\n    )\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer","title":"bs_roformer","text":"<p>Band-Split RoPE Transformer</p> <ul> <li>BS-RoFormer: https://arxiv.org/abs/2309.02612</li> <li>Mel-RoFormer: https://arxiv.org/abs/2409.04702</li> </ul> <p>This implementation merges the two versions found in <code>lucidrains</code>'s implementation However, there are several inconsistencies:</p> <ul> <li><code>MLP</code> was defined differently in each file, one that has <code>depth - 1</code> hidden layers and one that   has <code>depth</code> layers.</li> <li><code>BSRoformer</code> applies one final RMSNorm after the entire stack of transformer layers, while the   <code>MelBandRoformer</code> applies an RMSNorm at the end of each axial transformer block (time_transformer,   freq_transformer, etc.) and has no final normalization layer.</li> </ul> <p>Since fixing the three inconsistencies upstream is too big of a breaking change, we inherit them to maintain compatability with community-trained models. See: https://github.com/lucidrains/BS-RoFormer/issues/48.</p> <p>To avoid dependency bloat, we do not:</p> <ul> <li>depend on <code>rotary_embeddings_torch</code></li> <li>implement <code>hyper_connections</code></li> <li>implement learned value residual learning</li> </ul> <p>Classes:</p> Name Description <code>FixedBandsConfig</code> <code>MelBandsConfig</code> <code>BSRoformerParams</code> <code>RMSNorm</code> <code>RMSNormWithEps</code> <code>RotaryEmbedding</code> <p>A performance-oriented version of RoPE.</p> <code>FeedForward</code> <code>Attention</code> <code>LinearAttention</code> <p>this flavor of linear attention proposed in https://arxiv.org/abs/2106.09681 by El-Nouby et al.</p> <code>Transformer</code> <code>BandSplit</code> <code>MaskEstimator</code> <code>BSRoformer</code> <p>Functions:</p> Name Description <code>l2norm</code> <code>rms_norm</code> <code>mlp</code> <p>Attributes:</p> Name Type Description <code>DEFAULT_FREQS_PER_BANDS</code>"},{"location":"api/models/#splifft.models.bs_roformer.DEFAULT_FREQS_PER_BANDS","title":"DEFAULT_FREQS_PER_BANDS  <code>module-attribute</code>","text":"<pre><code>DEFAULT_FREQS_PER_BANDS = (\n    2,\n    2,\n    2,\n    2,\n    2,\n    2,\n    2,\n    2,\n    2,\n    2,\n    2,\n    2,\n    2,\n    2,\n    2,\n    2,\n    2,\n    2,\n    2,\n    2,\n    2,\n    2,\n    2,\n    2,\n    4,\n    4,\n    4,\n    4,\n    4,\n    4,\n    4,\n    4,\n    4,\n    4,\n    4,\n    4,\n    12,\n    12,\n    12,\n    12,\n    12,\n    12,\n    12,\n    12,\n    24,\n    24,\n    24,\n    24,\n    24,\n    24,\n    24,\n    24,\n    48,\n    48,\n    48,\n    48,\n    48,\n    48,\n    48,\n    48,\n    128,\n    129,\n)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.FixedBandsConfig","title":"FixedBandsConfig  <code>dataclass</code>","text":"<pre><code>FixedBandsConfig(\n    kind: Literal[\"fixed\"] = \"fixed\",\n    freqs_per_bands: tuple[Gt0[int], ...] = (\n        lambda: DEFAULT_FREQS_PER_BANDS\n    )(),\n)\n</code></pre> <p>Attributes:</p> Name Type Description <code>kind</code> <code>Literal['fixed']</code> <code>freqs_per_bands</code> <code>tuple[Gt0[int], ...]</code>"},{"location":"api/models/#splifft.models.bs_roformer.FixedBandsConfig.kind","title":"kind  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>kind: Literal['fixed'] = 'fixed'\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.FixedBandsConfig.freqs_per_bands","title":"freqs_per_bands  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>freqs_per_bands: tuple[Gt0[int], ...] = field(\n    default_factory=lambda: DEFAULT_FREQS_PER_BANDS\n)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.MelBandsConfig","title":"MelBandsConfig  <code>dataclass</code>","text":"<pre><code>MelBandsConfig(\n    kind: Literal[\"mel\"],\n    num_bands: Gt0[int],\n    sample_rate: Gt0[int],\n    stft_n_fft: Gt0[int],\n)\n</code></pre> <p>Attributes:</p> Name Type Description <code>kind</code> <code>Literal['mel']</code> <code>num_bands</code> <code>Gt0[int]</code> <code>sample_rate</code> <code>Gt0[int]</code> <code>stft_n_fft</code> <code>Gt0[int]</code>"},{"location":"api/models/#splifft.models.bs_roformer.MelBandsConfig.kind","title":"kind  <code>instance-attribute</code>","text":"<pre><code>kind: Literal['mel']\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.MelBandsConfig.num_bands","title":"num_bands  <code>instance-attribute</code>","text":"<pre><code>num_bands: Gt0[int]\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.MelBandsConfig.sample_rate","title":"sample_rate  <code>instance-attribute</code>","text":"<pre><code>sample_rate: Gt0[int]\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.MelBandsConfig.stft_n_fft","title":"stft_n_fft  <code>instance-attribute</code>","text":"<pre><code>stft_n_fft: Gt0[int]\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams","title":"BSRoformerParams  <code>dataclass</code>","text":"<pre><code>BSRoformerParams(\n    chunk_size: ChunkSize,\n    output_stem_names: tuple[ModelOutputStemName, ...],\n    dim: Gt0[int],\n    depth: Gt0[int],\n    stft_hop_length: HopSize,\n    stereo: bool = True,\n    time_transformer_depth: Gt0[int] = 1,\n    freq_transformer_depth: Gt0[int] = 1,\n    linear_transformer_depth: Ge0[int] = 0,\n    band_config: FixedBandsConfig\n    | MelBandsConfig = FixedBandsConfig(),\n    dim_head: int = 64,\n    heads: Gt0[int] = 8,\n    attn_dropout: Dropout = 0.0,\n    ff_dropout: Dropout = 0.0,\n    ff_mult: Gt0[int] = 4,\n    flash_attn: bool = True,\n    norm_output: bool = False,\n    mask_estimator_depth: Gt0[int] = 2,\n    mlp_expansion_factor: Gt0[int] = 4,\n    use_torch_checkpoint: bool = False,\n    sage_attention: bool = False,\n    use_shared_bias: bool = False,\n    skip_connection: bool = False,\n    rms_norm_eps: Ge0[float] | None = None,\n    rotary_embed_dtype: TorchDtype | None = None,\n    transformer_residual_dtype: TorchDtype | None = None,\n    debug: bool = False,\n)\n</code></pre> <p>               Bases: <code>ModelParamsLike</code></p> <p>Attributes:</p> Name Type Description <code>chunk_size</code> <code>ChunkSize</code> <code>output_stem_names</code> <code>tuple[ModelOutputStemName, ...]</code> <code>dim</code> <code>Gt0[int]</code> <code>depth</code> <code>Gt0[int]</code> <code>stft_hop_length</code> <code>HopSize</code> <code>stereo</code> <code>bool</code> <code>time_transformer_depth</code> <code>Gt0[int]</code> <code>freq_transformer_depth</code> <code>Gt0[int]</code> <code>linear_transformer_depth</code> <code>Ge0[int]</code> <code>band_config</code> <code>FixedBandsConfig | MelBandsConfig</code> <code>dim_head</code> <code>int</code> <code>heads</code> <code>Gt0[int]</code> <code>attn_dropout</code> <code>Dropout</code> <code>ff_dropout</code> <code>Dropout</code> <code>ff_mult</code> <code>Gt0[int]</code> <code>flash_attn</code> <code>bool</code> <code>norm_output</code> <code>bool</code> <p>Note that in <code>lucidrains</code>' implementation, this is set to</p> <code>mask_estimator_depth</code> <code>Gt0[int]</code> <p>The number of hidden layers of the MLP is <code>mask_estimator_depth - 1</code>, that is:</p> <code>mlp_expansion_factor</code> <code>Gt0[int]</code> <code>use_torch_checkpoint</code> <code>bool</code> <code>sage_attention</code> <code>bool</code> <code>use_shared_bias</code> <code>bool</code> <code>skip_connection</code> <code>bool</code> <code>rms_norm_eps</code> <code>Ge0[float] | None</code> <code>rotary_embed_dtype</code> <code>TorchDtype | None</code> <code>transformer_residual_dtype</code> <code>TorchDtype | None</code> <code>debug</code> <code>bool</code> <p>Whether to check for nan/inf in model outputs. Keep it off for torch.compile.</p> <code>input_type</code> <code>ModelInputType</code> <code>output_type</code> <code>ModelOutputType</code>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams.chunk_size","title":"chunk_size  <code>instance-attribute</code>","text":"<pre><code>chunk_size: ChunkSize\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams.output_stem_names","title":"output_stem_names  <code>instance-attribute</code>","text":"<pre><code>output_stem_names: tuple[ModelOutputStemName, ...]\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams.dim","title":"dim  <code>instance-attribute</code>","text":"<pre><code>dim: Gt0[int]\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams.depth","title":"depth  <code>instance-attribute</code>","text":"<pre><code>depth: Gt0[int]\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams.stft_hop_length","title":"stft_hop_length  <code>instance-attribute</code>","text":"<pre><code>stft_hop_length: HopSize\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams.stereo","title":"stereo  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>stereo: bool = True\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams.time_transformer_depth","title":"time_transformer_depth  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>time_transformer_depth: Gt0[int] = 1\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams.freq_transformer_depth","title":"freq_transformer_depth  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>freq_transformer_depth: Gt0[int] = 1\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams.linear_transformer_depth","title":"linear_transformer_depth  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>linear_transformer_depth: Ge0[int] = 0\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams.band_config","title":"band_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>band_config: FixedBandsConfig | MelBandsConfig = field(\n    default_factory=FixedBandsConfig\n)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams.dim_head","title":"dim_head  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>dim_head: int = 64\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams.heads","title":"heads  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>heads: Gt0[int] = 8\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams.attn_dropout","title":"attn_dropout  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>attn_dropout: Dropout = 0.0\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams.ff_dropout","title":"ff_dropout  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ff_dropout: Dropout = 0.0\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams.ff_mult","title":"ff_mult  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ff_mult: Gt0[int] = 4\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams.flash_attn","title":"flash_attn  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>flash_attn: bool = True\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams.norm_output","title":"norm_output  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>norm_output: bool = False\n</code></pre> <p>Note that in <code>lucidrains</code>' implementation, this is set to False for <code>bs_roformer</code> but True for <code>mel_roformer</code>!!</p>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams.mask_estimator_depth","title":"mask_estimator_depth  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mask_estimator_depth: Gt0[int] = 2\n</code></pre> <p>The number of hidden layers of the MLP is <code>mask_estimator_depth - 1</code>, that is:</p> <ul> <li>depth = 1: (dim_in, dim_out)</li> <li>depth = 2: (dim_in, dim_hidden, dim_out)</li> </ul> <p>Note that in <code>lucidrains</code>' implementation of mel-band roformers, the number of hidden layers is incorrectly set as <code>mask_estimator_depth</code>. This includes popular models like kim-vocals and all models that use <code>zfturbo</code>'s music-source-separation training.</p> <p>If you are migrating a mel-band roformer's <code>zfturbo</code> configuration, increment the mask_estimator depth by 1.</p>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams.mlp_expansion_factor","title":"mlp_expansion_factor  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mlp_expansion_factor: Gt0[int] = 4\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams.use_torch_checkpoint","title":"use_torch_checkpoint  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>use_torch_checkpoint: bool = False\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams.sage_attention","title":"sage_attention  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>sage_attention: bool = False\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams.use_shared_bias","title":"use_shared_bias  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>use_shared_bias: bool = False\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams.skip_connection","title":"skip_connection  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>skip_connection: bool = False\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams.rms_norm_eps","title":"rms_norm_eps  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>rms_norm_eps: Ge0[float] | None = None\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams.rotary_embed_dtype","title":"rotary_embed_dtype  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>rotary_embed_dtype: TorchDtype | None = None\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams.transformer_residual_dtype","title":"transformer_residual_dtype  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>transformer_residual_dtype: TorchDtype | None = None\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams.debug","title":"debug  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>debug: bool = False\n</code></pre> <p>Whether to check for nan/inf in model outputs. Keep it off for torch.compile.</p>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams.input_type","title":"input_type  <code>property</code>","text":"<pre><code>input_type: ModelInputType\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams.output_type","title":"output_type  <code>property</code>","text":"<pre><code>output_type: ModelOutputType\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.l2norm","title":"l2norm","text":"<pre><code>l2norm(t: Tensor) -&gt; Tensor\n</code></pre> Source code in <code>src/splifft/models/bs_roformer.py</code> <pre><code>def l2norm(t: Tensor) -&gt; Tensor:\n    return F.normalize(t, dim=-1, p=2)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.RMSNorm","title":"RMSNorm","text":"<pre><code>RMSNorm(dim: int)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>scale</code> <code>gamma</code> Source code in <code>src/splifft/models/bs_roformer.py</code> <pre><code>def __init__(self, dim: int):\n    super().__init__()\n    self.scale = dim**0.5\n    self.gamma = nn.Parameter(torch.ones(dim))\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.RMSNorm.scale","title":"scale  <code>instance-attribute</code>","text":"<pre><code>scale = dim ** 0.5\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.RMSNorm.gamma","title":"gamma  <code>instance-attribute</code>","text":"<pre><code>gamma = Parameter(ones(dim))\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.RMSNorm.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> Source code in <code>src/splifft/models/bs_roformer.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    return F.normalize(x, dim=-1) * self.scale * self.gamma  # type: ignore\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.RMSNormWithEps","title":"RMSNormWithEps","text":"<pre><code>RMSNormWithEps(\n    dim: int, eps: float = 5.960464477539063e-08\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>scale</code> <code>gamma</code> <code>eps</code> Source code in <code>src/splifft/models/bs_roformer.py</code> <pre><code>def __init__(self, dim: int, eps: float = 5.960464477539063e-08):\n    super().__init__()\n    self.scale = dim**0.5\n    self.gamma = nn.Parameter(torch.ones(dim))\n    self.eps = eps\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.RMSNormWithEps.scale","title":"scale  <code>instance-attribute</code>","text":"<pre><code>scale = dim ** 0.5\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.RMSNormWithEps.gamma","title":"gamma  <code>instance-attribute</code>","text":"<pre><code>gamma = Parameter(ones(dim))\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.RMSNormWithEps.eps","title":"eps  <code>instance-attribute</code>","text":"<pre><code>eps = eps\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.RMSNormWithEps.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> Source code in <code>src/splifft/models/bs_roformer.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    l2_norm = torch.linalg.norm(x, dim=-1, keepdim=True)\n    denom = torch.maximum(l2_norm, torch.full_like(l2_norm, self.eps))\n    normalized_x = x / denom\n    return normalized_x * self.scale * self.gamma  # type: ignore\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.rms_norm","title":"rms_norm","text":"<pre><code>rms_norm(\n    dim: int, eps: float | None\n) -&gt; RMSNorm | RMSNormWithEps\n</code></pre> Source code in <code>src/splifft/models/bs_roformer.py</code> <pre><code>def rms_norm(dim: int, eps: float | None) -&gt; RMSNorm | RMSNormWithEps:\n    if eps is None:\n        return RMSNorm(dim)\n    return RMSNormWithEps(dim, eps)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.RotaryEmbedding","title":"RotaryEmbedding","text":"<pre><code>RotaryEmbedding(\n    seq_len: int,\n    dim_head: int,\n    *,\n    dtype: dtype | None,\n    theta: int = 10000,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>A performance-oriented version of RoPE.</p> <p>Unlike <code>lucidrains</code>' implementation which compute embeddings JIT during the forward pass and caches calls with the same or shorter sequence length, we simply compute them AOT as persistent buffers. To keep the computational graph clean, we do not support dynamic sequence lengths, learned frequencies or length extrapolation.</p> <p>Methods:</p> Name Description <code>rotate_half</code> <code>forward</code> <p>Attributes:</p> Name Type Description <code>cos_emb</code> <code>sin_emb</code> Source code in <code>src/splifft/models/bs_roformer.py</code> <pre><code>def __init__(\n    self, seq_len: int, dim_head: int, *, dtype: torch.dtype | None, theta: int = 10000\n):\n    super().__init__()\n    # COMPAT: the original implementation does not generate the embeddings\n    # on the fly, but serialises them in fp16. there are some tiny\n    # differences:\n    # |                     |   from weights  |   generated    |\n    # | ------------------- | --------------- | -------------- |\n    # | cos_emb_time:971,22 | -0.99462890625  | -0.994140625   |\n    # | cos_emb_time:971,23 | -0.99462890625  | -0.994140625   |\n    # | sin_emb_time:727,12 | -0.457763671875 | -0.4580078125  |\n    # | sin_emb_time:727,13 | -0.457763671875 | -0.4580078125  |\n    # | sin_emb_time:825,4  | -0.8544921875   | -0.85400390625 |\n    # | sin_emb_time:825,5  | -0.8544921875   | -0.85400390625 |\n    freqs = 1.0 / (theta ** (torch.arange(0, dim_head, 2).float() / dim_head))\n    t = torch.arange(seq_len)\n    freqs = torch.einsum(\"i,j-&gt;ij\", t, freqs)  # (seq_len, dim / 2)\n    freqs = repeat(freqs, \"... d -&gt; ... (d r)\", r=2)  # (seq_len, dim)\n    self.cos_emb = freqs.cos().to(dtype)\n    self.sin_emb = freqs.sin().to(dtype)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.RotaryEmbedding.cos_emb","title":"cos_emb  <code>instance-attribute</code>","text":"<pre><code>cos_emb = to(dtype)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.RotaryEmbedding.sin_emb","title":"sin_emb  <code>instance-attribute</code>","text":"<pre><code>sin_emb = to(dtype)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.RotaryEmbedding.rotate_half","title":"rotate_half","text":"<pre><code>rotate_half(x: Tensor) -&gt; Tensor\n</code></pre> Source code in <code>src/splifft/models/bs_roformer.py</code> <pre><code>def rotate_half(self, x: Tensor) -&gt; Tensor:\n    x = rearrange(x, \"... (d r) -&gt; ... d r\", r=2)\n    x1, x2 = x.unbind(dim=-1)\n    x = torch.stack((-x2, x1), dim=-1)\n    return rearrange(x, \"... d r -&gt; ... (d r)\")\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.RotaryEmbedding.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> Source code in <code>src/splifft/models/bs_roformer.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    # x is (batch_eff, heads, seq_len_for_rotation, dim_head)\n    cos_b = self.cos_emb.unsqueeze(0).unsqueeze(0).to(x.device, x.dtype)\n    sin_b = self.sin_emb.unsqueeze(0).unsqueeze(0).to(x.device, x.dtype)\n\n    term1 = x * cos_b\n    term2 = self.rotate_half(x) * sin_b\n\n    # NOTE: original impl performed addition between two f32s but it comes with 30% slowdown\n    # we eliminate it so the addition is performed between two f16s (according to __init__).\n    return term1 + term2\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.FeedForward","title":"FeedForward","text":"<pre><code>FeedForward(\n    dim: int,\n    mult: int = 4,\n    dropout: float = 0.0,\n    rms_norm_eps: float | None = None,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>net</code> Source code in <code>src/splifft/models/bs_roformer.py</code> <pre><code>def __init__(\n    self, dim: int, mult: int = 4, dropout: float = 0.0, rms_norm_eps: float | None = None\n):\n    super().__init__()\n    dim_inner = int(dim * mult)\n    # NOTE: in the paper: RMSNorm -&gt; FC -&gt; Tanh -&gt; FC -&gt; GLU\n    self.net = nn.Sequential(\n        rms_norm(dim, eps=rms_norm_eps),\n        nn.Linear(dim, dim_inner),\n        nn.GELU(),\n        nn.Dropout(dropout),\n        nn.Linear(dim_inner, dim),\n        nn.Dropout(dropout),\n    )\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.FeedForward.net","title":"net  <code>instance-attribute</code>","text":"<pre><code>net = Sequential(\n    rms_norm(dim, eps=rms_norm_eps),\n    Linear(dim, dim_inner),\n    GELU(),\n    Dropout(dropout),\n    Linear(dim_inner, dim),\n    Dropout(dropout),\n)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.FeedForward.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> Source code in <code>src/splifft/models/bs_roformer.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    return self.net(x)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.Attention","title":"Attention","text":"<pre><code>Attention(\n    dim: int,\n    heads: int = 8,\n    dim_head: int = 64,\n    dropout: float = 0.0,\n    shared_qkv_bias: Parameter | None = None,\n    shared_out_bias: Parameter | None = None,\n    rotary_embed: RotaryEmbedding | None = None,\n    flash: bool = True,\n    sage_attention: bool = False,\n    rms_norm_eps: float | None = None,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>heads</code> <code>scale</code> <code>rotary_embed</code> <code>attend</code> <code>norm</code> <code>to_qkv</code> <code>to_gates</code> <code>to_out</code> Source code in <code>src/splifft/models/bs_roformer.py</code> <pre><code>def __init__(\n    self,\n    dim: int,\n    heads: int = 8,\n    dim_head: int = 64,\n    dropout: float = 0.0,\n    shared_qkv_bias: nn.Parameter | None = None,\n    shared_out_bias: nn.Parameter | None = None,\n    rotary_embed: RotaryEmbedding | None = None,\n    flash: bool = True,\n    sage_attention: bool = False,\n    rms_norm_eps: float | None = None,\n):\n    super().__init__()\n    self.heads = heads\n    self.scale = dim_head**-0.5\n    dim_inner = heads * dim_head\n\n    self.rotary_embed = rotary_embed\n\n    if sage_attention:\n        from .utils.attend_sage import AttendSage\n\n        self.attend = AttendSage(flash=flash, dropout=dropout)\n    else:\n        from .utils.attend import Attend\n\n        self.attend = Attend(flash=flash, dropout=dropout)  # type: ignore\n\n    self.norm = rms_norm(dim, eps=rms_norm_eps)\n    self.to_qkv = nn.Linear(dim, dim_inner * 3, bias=(shared_qkv_bias is not None))\n    if shared_qkv_bias is not None:\n        self.to_qkv.bias = shared_qkv_bias\n\n    self.to_gates = nn.Linear(dim, heads)\n\n    self.to_out = nn.Sequential(\n        nn.Linear(dim_inner, dim, bias=(shared_out_bias is not None)),\n        nn.Dropout(dropout),\n    )\n    if shared_out_bias is not None:\n        self.to_out[0].bias = shared_out_bias\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.Attention.heads","title":"heads  <code>instance-attribute</code>","text":"<pre><code>heads = heads\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.Attention.scale","title":"scale  <code>instance-attribute</code>","text":"<pre><code>scale = dim_head ** -0.5\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.Attention.rotary_embed","title":"rotary_embed  <code>instance-attribute</code>","text":"<pre><code>rotary_embed = rotary_embed\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.Attention.attend","title":"attend  <code>instance-attribute</code>","text":"<pre><code>attend = AttendSage(flash=flash, dropout=dropout)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.Attention.norm","title":"norm  <code>instance-attribute</code>","text":"<pre><code>norm = rms_norm(dim, eps=rms_norm_eps)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.Attention.to_qkv","title":"to_qkv  <code>instance-attribute</code>","text":"<pre><code>to_qkv = Linear(\n    dim, dim_inner * 3, bias=shared_qkv_bias is not None\n)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.Attention.to_gates","title":"to_gates  <code>instance-attribute</code>","text":"<pre><code>to_gates = Linear(dim, heads)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.Attention.to_out","title":"to_out  <code>instance-attribute</code>","text":"<pre><code>to_out = Sequential(\n    Linear(\n        dim_inner, dim, bias=shared_out_bias is not None\n    ),\n    Dropout(dropout),\n)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.Attention.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> Source code in <code>src/splifft/models/bs_roformer.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    x = self.norm(x)\n\n    qkv = self.to_qkv(x)\n    q, k, v = rearrange(qkv, \"b n (qkv h d) -&gt; qkv b h n d\", qkv=3, h=self.heads)\n\n    if self.rotary_embed is not None:\n        q = self.rotary_embed(q)\n        k = self.rotary_embed(k)\n\n    out = self.attend(q, k, v)\n\n    gates = self.to_gates(x)\n    gate_act = gates.sigmoid()\n\n    out = out * rearrange(gate_act, \"b n h -&gt; b h n 1\")\n\n    out = rearrange(out, \"b h n d -&gt; b n (h d)\")\n    out = self.to_out(out)\n    return out\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.LinearAttention","title":"LinearAttention","text":"<pre><code>LinearAttention(\n    *,\n    dim: int,\n    dim_head: int = 32,\n    heads: int = 8,\n    scale: int = 8,\n    flash: bool = False,\n    dropout: float = 0.0,\n    sage_attention: bool = False,\n    rms_norm_eps: float | None = None,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>this flavor of linear attention proposed in https://arxiv.org/abs/2106.09681 by El-Nouby et al.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>norm</code> <code>to_qkv</code> <code>temperature</code> <code>attend</code> <code>to_out</code> Source code in <code>src/splifft/models/bs_roformer.py</code> <pre><code>def __init__(\n    self,\n    *,\n    dim: int,\n    dim_head: int = 32,\n    heads: int = 8,\n    scale: int = 8,\n    flash: bool = False,\n    dropout: float = 0.0,\n    sage_attention: bool = False,\n    rms_norm_eps: float | None = None,\n):\n    super().__init__()\n    dim_inner = dim_head * heads\n    self.norm = rms_norm(dim, eps=rms_norm_eps)\n\n    self.to_qkv = nn.Sequential(\n        nn.Linear(dim, dim_inner * 3, bias=False),\n        Rearrange(\"b n (qkv h d) -&gt; qkv b h d n\", qkv=3, h=heads),\n    )\n\n    self.temperature = nn.Parameter(torch.ones(heads, 1, 1))\n\n    if sage_attention:\n        from .utils.attend_sage import AttendSage\n\n        self.attend = AttendSage(scale=scale, dropout=dropout, flash=flash)\n    else:\n        from .utils.attend import Attend\n\n        self.attend = Attend(scale=scale, dropout=dropout, flash=flash)  # type: ignore\n\n    self.to_out = nn.Sequential(\n        Rearrange(\"b h d n -&gt; b n (h d)\"), nn.Linear(dim_inner, dim, bias=False)\n    )\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.LinearAttention.norm","title":"norm  <code>instance-attribute</code>","text":"<pre><code>norm = rms_norm(dim, eps=rms_norm_eps)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.LinearAttention.to_qkv","title":"to_qkv  <code>instance-attribute</code>","text":"<pre><code>to_qkv = Sequential(\n    Linear(dim, dim_inner * 3, bias=False),\n    Rearrange(\n        \"b n (qkv h d) -&gt; qkv b h d n\", qkv=3, h=heads\n    ),\n)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.LinearAttention.temperature","title":"temperature  <code>instance-attribute</code>","text":"<pre><code>temperature = Parameter(ones(heads, 1, 1))\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.LinearAttention.attend","title":"attend  <code>instance-attribute</code>","text":"<pre><code>attend = AttendSage(\n    scale=scale, dropout=dropout, flash=flash\n)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.LinearAttention.to_out","title":"to_out  <code>instance-attribute</code>","text":"<pre><code>to_out = Sequential(\n    Rearrange(\"b h d n -&gt; b n (h d)\"),\n    Linear(dim_inner, dim, bias=False),\n)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.LinearAttention.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> Source code in <code>src/splifft/models/bs_roformer.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    x = self.norm(x)\n\n    q, k, v = self.to_qkv(x)\n\n    q, k = map(l2norm, (q, k))\n    q = q * self.temperature.exp()\n\n    out = self.attend(q, k, v)\n\n    return self.to_out(out)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.Transformer","title":"Transformer","text":"<pre><code>Transformer(\n    *,\n    dim: int,\n    depth: int,\n    dim_head: int = 64,\n    heads: int = 8,\n    attn_dropout: float = 0.0,\n    ff_dropout: float = 0.0,\n    ff_mult: int = 4,\n    norm_output: bool = True,\n    rotary_embed: RotaryEmbedding | None = None,\n    flash_attn: bool = True,\n    linear_attn: bool = False,\n    sage_attention: bool = False,\n    shared_qkv_bias: Parameter | None = None,\n    shared_out_bias: Parameter | None = None,\n    rms_norm_eps: float | None = None,\n    transformer_residual_dtype: dtype | None = None,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>layers</code> <code>transformer_residual_dtype</code> <code>norm</code> Source code in <code>src/splifft/models/bs_roformer.py</code> <pre><code>def __init__(\n    self,\n    *,\n    dim: int,\n    depth: int,\n    dim_head: int = 64,\n    heads: int = 8,\n    attn_dropout: float = 0.0,\n    ff_dropout: float = 0.0,\n    ff_mult: int = 4,\n    norm_output: bool = True,\n    rotary_embed: RotaryEmbedding | None = None,\n    flash_attn: bool = True,\n    linear_attn: bool = False,\n    sage_attention: bool = False,\n    shared_qkv_bias: nn.Parameter | None = None,\n    shared_out_bias: nn.Parameter | None = None,\n    rms_norm_eps: float | None = None,\n    transformer_residual_dtype: torch.dtype | None = None,  # COMPAT: float32, see 265\n):\n    super().__init__()\n    self.layers = ModuleList([])\n\n    for _ in range(depth):\n        attn: LinearAttention | Attention\n        if linear_attn:\n            attn = LinearAttention(\n                dim=dim,\n                dim_head=dim_head,\n                heads=heads,\n                dropout=attn_dropout,\n                flash=flash_attn,\n                sage_attention=sage_attention,\n                rms_norm_eps=rms_norm_eps,\n            )\n        else:\n            attn = Attention(\n                dim=dim,\n                dim_head=dim_head,\n                heads=heads,\n                dropout=attn_dropout,\n                shared_qkv_bias=shared_qkv_bias,\n                shared_out_bias=shared_out_bias,\n                rotary_embed=rotary_embed,\n                flash=flash_attn,\n                sage_attention=sage_attention,\n                rms_norm_eps=rms_norm_eps,\n            )\n\n        ff = FeedForward(dim=dim, mult=ff_mult, dropout=ff_dropout, rms_norm_eps=rms_norm_eps)\n        self.layers.append(ModuleList([attn, ff]))\n    self.transformer_residual_dtype = transformer_residual_dtype\n\n    self.norm = rms_norm(dim, eps=rms_norm_eps) if norm_output else nn.Identity()\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.Transformer.layers","title":"layers  <code>instance-attribute</code>","text":"<pre><code>layers = ModuleList([])\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.Transformer.transformer_residual_dtype","title":"transformer_residual_dtype  <code>instance-attribute</code>","text":"<pre><code>transformer_residual_dtype = transformer_residual_dtype\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.Transformer.norm","title":"norm  <code>instance-attribute</code>","text":"<pre><code>norm = (\n    rms_norm(dim, eps=rms_norm_eps)\n    if norm_output\n    else Identity()\n)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.Transformer.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> Source code in <code>src/splifft/models/bs_roformer.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    for attn, ff in self.layers:  # type: ignore\n        attn_out = attn(x)\n        if self.transformer_residual_dtype is not None:\n            x = (\n                attn_out.to(self.transformer_residual_dtype)\n                + x.to(self.transformer_residual_dtype)\n            ).to(x.dtype)\n        else:\n            x = attn_out + x\n\n        ff_out = ff(x)\n        if self.transformer_residual_dtype is not None:\n            x = (\n                ff_out.to(self.transformer_residual_dtype)\n                + x.to(self.transformer_residual_dtype)\n            ).to(x.dtype)\n        else:\n            x = ff_out + x\n    return self.norm(x)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BandSplit","title":"BandSplit","text":"<pre><code>BandSplit(\n    dim: int,\n    dim_inputs: tuple[int, ...],\n    rms_norm_eps: float | None = None,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>dim_inputs</code> <code>to_features</code> Source code in <code>src/splifft/models/bs_roformer.py</code> <pre><code>def __init__(self, dim: int, dim_inputs: tuple[int, ...], rms_norm_eps: float | None = None):\n    super().__init__()\n    self.dim_inputs = dim_inputs\n    self.to_features = ModuleList([])\n\n    for dim_in in dim_inputs:\n        net = nn.Sequential(rms_norm(dim_in, rms_norm_eps), nn.Linear(dim_in, dim))\n        self.to_features.append(net)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BandSplit.dim_inputs","title":"dim_inputs  <code>instance-attribute</code>","text":"<pre><code>dim_inputs = dim_inputs\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BandSplit.to_features","title":"to_features  <code>instance-attribute</code>","text":"<pre><code>to_features = ModuleList([])\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BandSplit.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> Source code in <code>src/splifft/models/bs_roformer.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    x_split = x.split(self.dim_inputs, dim=-1)\n    outs = []\n    for split_input, to_feature_net in zip(x_split, self.to_features):\n        split_output = to_feature_net(split_input)\n        outs.append(split_output)\n    return torch.stack(outs, dim=-2)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.mlp","title":"mlp","text":"<pre><code>mlp(\n    dim_in: int,\n    dim_out: int,\n    dim_hidden: int | None = None,\n    depth: int = 1,\n    activation: type[Module] = Tanh,\n) -&gt; Sequential\n</code></pre> Source code in <code>src/splifft/models/bs_roformer.py</code> <pre><code>def mlp(\n    dim_in: int,\n    dim_out: int,\n    dim_hidden: int | None = None,\n    depth: int = 1,\n    activation: type[Module] = nn.Tanh,\n) -&gt; nn.Sequential:\n    dim_hidden_ = dim_hidden or dim_in\n\n    net: list[Module] = []\n    # NOTE: in lucidrain's impl, `bs_roformer` has `depth - 1` but `mel_roformer` has `depth`\n    num_hidden_layers = depth - 1\n    dims = (dim_in, *((dim_hidden_,) * num_hidden_layers), dim_out)\n\n    for ind, (layer_dim_in, layer_dim_out) in enumerate(zip(dims[:-1], dims[1:])):\n        is_last = ind == (len(dims) - 2)\n\n        net.append(nn.Linear(layer_dim_in, layer_dim_out))\n\n        if is_last:\n            continue\n\n        net.append(activation())\n\n    return nn.Sequential(*net)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.MaskEstimator","title":"MaskEstimator","text":"<pre><code>MaskEstimator(\n    dim: int,\n    dim_inputs: tuple[int, ...],\n    depth: int,\n    mlp_expansion_factor: int,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>dim_inputs</code> <code>to_freqs</code> Source code in <code>src/splifft/models/bs_roformer.py</code> <pre><code>def __init__(\n    self,\n    dim: int,\n    dim_inputs: tuple[int, ...],\n    depth: int,\n    mlp_expansion_factor: int,\n):\n    super().__init__()\n    self.dim_inputs = dim_inputs\n    self.to_freqs = ModuleList([])\n    dim_hidden = dim * mlp_expansion_factor\n\n    for dim_in in dim_inputs:\n        self.to_freqs.append(\n            nn.Sequential(\n                mlp(dim, dim_in * 2, dim_hidden=dim_hidden, depth=depth),\n                nn.GLU(dim=-1),\n            )\n        )\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.MaskEstimator.dim_inputs","title":"dim_inputs  <code>instance-attribute</code>","text":"<pre><code>dim_inputs = dim_inputs\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.MaskEstimator.to_freqs","title":"to_freqs  <code>instance-attribute</code>","text":"<pre><code>to_freqs = ModuleList([])\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.MaskEstimator.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> Source code in <code>src/splifft/models/bs_roformer.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    x_unbound = x.unbind(dim=-2)\n\n    outs = []\n\n    for band_features, mlp_net in zip(x_unbound, self.to_freqs):\n        freq_out = mlp_net(band_features)\n        outs.append(freq_out)\n\n    return torch.cat(outs, dim=-1)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformer","title":"BSRoformer","text":"<pre><code>BSRoformer(cfg: BSRoformerParams)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Methods:</p> Name Description <code>forward</code> <p>:param stft_repr: input spectrogram. shape (b, f*s, t, c)</p> <p>Attributes:</p> Name Type Description <code>stereo</code> <code>audio_channels</code> <code>num_stems</code> <code>use_torch_checkpoint</code> <code>skip_connection</code> <code>layers</code> <code>shared_qkv_bias</code> <code>Parameter | None</code> <code>shared_out_bias</code> <code>Parameter | None</code> <code>is_mel</code> <code>final_norm</code> <code>band_split</code> <code>mask_estimators</code> <code>debug</code> Source code in <code>src/splifft/models/bs_roformer.py</code> <pre><code>def __init__(self, cfg: BSRoformerParams):\n    super().__init__()\n    self.stereo = cfg.stereo\n    self.audio_channels = 2 if cfg.stereo else 1\n    self.num_stems = len(cfg.output_stem_names)\n    self.use_torch_checkpoint = cfg.use_torch_checkpoint\n    self.skip_connection = cfg.skip_connection\n\n    self.layers = ModuleList([])\n\n    self.shared_qkv_bias: nn.Parameter | None = None\n    self.shared_out_bias: nn.Parameter | None = None\n    if cfg.use_shared_bias:\n        dim_inner = cfg.heads * cfg.dim_head\n        self.shared_qkv_bias = nn.Parameter(torch.ones(dim_inner * 3))\n        self.shared_out_bias = nn.Parameter(torch.ones(cfg.dim))\n\n    transformer = partial(\n        Transformer,\n        dim=cfg.dim,\n        heads=cfg.heads,\n        dim_head=cfg.dim_head,\n        attn_dropout=cfg.attn_dropout,\n        ff_dropout=cfg.ff_dropout,\n        ff_mult=cfg.ff_mult,\n        flash_attn=cfg.flash_attn,\n        norm_output=cfg.norm_output,\n        sage_attention=cfg.sage_attention,\n        shared_qkv_bias=self.shared_qkv_bias,\n        shared_out_bias=self.shared_out_bias,\n        rms_norm_eps=cfg.rms_norm_eps,\n        transformer_residual_dtype=cfg.transformer_residual_dtype,\n    )\n\n    t_frames = cfg.chunk_size // cfg.stft_hop_length + 1  # e.g. 588800 // 512 + 1 = 1151\n    time_rotary_embed = RotaryEmbedding(\n        seq_len=t_frames, dim_head=cfg.dim_head, dtype=cfg.rotary_embed_dtype\n    )\n\n    if is_mel := isinstance(cfg.band_config, MelBandsConfig):\n        from torchaudio.functional import melscale_fbanks\n\n        mel_cfg = cfg.band_config\n        num_bands = mel_cfg.num_bands\n        freqs = mel_cfg.stft_n_fft // 2 + 1\n        mel_filter_bank = melscale_fbanks(\n            n_freqs=freqs,\n            f_min=0.0,\n            f_max=float(mel_cfg.sample_rate / 2),\n            n_mels=num_bands,\n            sample_rate=mel_cfg.sample_rate,\n            norm=\"slaney\",\n            mel_scale=\"slaney\",\n        ).T\n        # TODO: adopt https://github.com/lucidrains/BS-RoFormer/issues/47\n        mel_filter_bank[0, 0] = 1.0\n        mel_filter_bank[-1, -1] = 1.0\n\n        freqs_per_band_mask = mel_filter_bank &gt; 0\n        assert freqs_per_band_mask.any(dim=0).all(), (\n            \"all frequencies must be covered by at least one band\"\n        )\n\n        repeated_freq_indices = repeat(torch.arange(freqs), \"f -&gt; b f\", b=num_bands)\n        freq_indices = repeated_freq_indices[freqs_per_band_mask]\n        if self.stereo:\n            freq_indices = repeat(freq_indices, \"f -&gt; f s\", s=2)\n            freq_indices = freq_indices * 2 + torch.arange(2)\n            freq_indices = rearrange(freq_indices, \"f s -&gt; (f s)\")\n        self.register_buffer(\"freq_indices\", freq_indices, persistent=False)\n        self.register_buffer(\"freqs_per_band_mask\", freqs_per_band_mask, persistent=False)\n\n        num_freqs_per_band = reduce(freqs_per_band_mask, \"b f -&gt; b\", \"sum\")\n        num_bands_per_freq = reduce(freqs_per_band_mask, \"b f -&gt; f\", \"sum\")\n\n        self.register_buffer(\"num_freqs_per_band\", num_freqs_per_band, persistent=False)\n        self.register_buffer(\"num_bands_per_freq\", num_bands_per_freq, persistent=False)\n\n    elif isinstance(cfg.band_config, FixedBandsConfig):\n        num_freqs_per_band = torch.tensor(cfg.band_config.freqs_per_bands)\n        num_bands = len(cfg.band_config.freqs_per_bands)\n    else:\n        raise TypeError(f\"unknown band config: {cfg.band_config}\")\n    self.is_mel = is_mel\n\n    freq_rotary_embed = RotaryEmbedding(\n        seq_len=num_bands, dim_head=cfg.dim_head, dtype=cfg.rotary_embed_dtype\n    )\n\n    for _ in range(cfg.depth):\n        tran_modules = []\n        if cfg.linear_transformer_depth &gt; 0:\n            tran_modules.append(\n                transformer(depth=cfg.linear_transformer_depth, linear_attn=True)\n            )\n        tran_modules.append(\n            transformer(depth=cfg.time_transformer_depth, rotary_embed=time_rotary_embed)\n        )\n        tran_modules.append(\n            transformer(depth=cfg.freq_transformer_depth, rotary_embed=freq_rotary_embed)\n        )\n        self.layers.append(nn.ModuleList(tran_modules))\n\n    self.final_norm = (\n        rms_norm(cfg.dim, eps=cfg.rms_norm_eps) if not self.is_mel else nn.Identity()\n    )\n\n    freqs_per_bands_with_complex = tuple(\n        2 * f * self.audio_channels for f in num_freqs_per_band.tolist()\n    )\n\n    self.band_split = BandSplit(\n        dim=cfg.dim,\n        dim_inputs=freqs_per_bands_with_complex,\n        rms_norm_eps=cfg.rms_norm_eps,\n    )\n\n    self.mask_estimators = nn.ModuleList([])\n\n    for _ in range(len(cfg.output_stem_names)):\n        mask_estimator = MaskEstimator(\n            dim=cfg.dim,\n            dim_inputs=freqs_per_bands_with_complex,\n            depth=cfg.mask_estimator_depth,\n            mlp_expansion_factor=cfg.mlp_expansion_factor,\n        )\n\n        self.mask_estimators.append(mask_estimator)\n\n    self.debug = cfg.debug\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformer.stereo","title":"stereo  <code>instance-attribute</code>","text":"<pre><code>stereo = stereo\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformer.audio_channels","title":"audio_channels  <code>instance-attribute</code>","text":"<pre><code>audio_channels = 2 if stereo else 1\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformer.num_stems","title":"num_stems  <code>instance-attribute</code>","text":"<pre><code>num_stems = len(output_stem_names)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformer.use_torch_checkpoint","title":"use_torch_checkpoint  <code>instance-attribute</code>","text":"<pre><code>use_torch_checkpoint = use_torch_checkpoint\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformer.skip_connection","title":"skip_connection  <code>instance-attribute</code>","text":"<pre><code>skip_connection = skip_connection\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformer.layers","title":"layers  <code>instance-attribute</code>","text":"<pre><code>layers = ModuleList([])\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformer.shared_qkv_bias","title":"shared_qkv_bias  <code>instance-attribute</code>","text":"<pre><code>shared_qkv_bias: Parameter | None = None\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformer.shared_out_bias","title":"shared_out_bias  <code>instance-attribute</code>","text":"<pre><code>shared_out_bias: Parameter | None = None\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformer.is_mel","title":"is_mel  <code>instance-attribute</code>","text":"<pre><code>is_mel = is_mel\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformer.final_norm","title":"final_norm  <code>instance-attribute</code>","text":"<pre><code>final_norm = (\n    rms_norm(dim, eps=rms_norm_eps)\n    if not is_mel\n    else Identity()\n)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformer.band_split","title":"band_split  <code>instance-attribute</code>","text":"<pre><code>band_split = BandSplit(\n    dim=dim,\n    dim_inputs=freqs_per_bands_with_complex,\n    rms_norm_eps=rms_norm_eps,\n)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformer.mask_estimators","title":"mask_estimators  <code>instance-attribute</code>","text":"<pre><code>mask_estimators = ModuleList([])\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformer.debug","title":"debug  <code>instance-attribute</code>","text":"<pre><code>debug = debug\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformer.forward","title":"forward","text":"<pre><code>forward(stft_repr: Tensor) -&gt; Tensor\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>stft_repr</code> <code>Tensor</code> <p>input spectrogram. shape (b, f*s, t, c)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>estimated mask. shape (b, n, f*s, t, c)</p> Source code in <code>src/splifft/models/bs_roformer.py</code> <pre><code>def forward(self, stft_repr: Tensor) -&gt; Tensor:\n    \"\"\"\n    :param stft_repr: input spectrogram. shape (b, f*s, t, c)\n    :return: estimated mask. shape (b, n, f*s, t, c)\n    \"\"\"\n    batch, _, t_frames, _ = stft_repr.shape\n    device = stft_repr.device\n    if self.is_mel:\n        batch_arange = torch.arange(batch, device=device)[..., None]\n        x = stft_repr[batch_arange, self.freq_indices]\n        x = rearrange(x, \"b f t c -&gt; b t (f c)\")\n    else:\n        x = rearrange(stft_repr, \"b f t c -&gt; b t (f c)\")\n\n    if self.debug and (torch.isnan(x).any() or torch.isinf(x).any()):\n        raise RuntimeError(\n            f\"nan/inf in x after rearrange: {x.isnan().sum()} nans, {x.isinf().sum()} infs\"\n        )\n\n    if self.use_torch_checkpoint:\n        x = checkpoint(self.band_split, x, use_reentrant=False)\n    else:\n        x = self.band_split(x)\n\n    if self.debug and (torch.isnan(x).any() or torch.isinf(x).any()):\n        raise RuntimeError(\n            f\"nan/inf in x after band_split: {x.isnan().sum()} nans, {x.isinf().sum()} infs\"\n        )\n\n    # axial / hierarchical attention\n\n    store = [None] * len(self.layers)\n    for i, transformer_block in enumerate(self.layers):\n        if len(transformer_block) == 3:\n            linear_transformer, time_transformer, freq_transformer = transformer_block\n\n            x, ft_ps = pack([x], \"b * d\")\n            if self.use_torch_checkpoint:\n                x = checkpoint(linear_transformer, x, use_reentrant=False)\n            else:\n                x = linear_transformer(x)\n            (x,) = unpack(x, ft_ps, \"b * d\")\n        else:\n            time_transformer, freq_transformer = transformer_block\n\n        if self.skip_connection:\n            for j in range(i):\n                x = x + store[j]\n\n        x = rearrange(x, \"b t f d -&gt; b f t d\")\n        x, ps = pack([x], \"* t d\")\n\n        if self.use_torch_checkpoint:\n            x = checkpoint(time_transformer, x, use_reentrant=False)\n        else:\n            x = time_transformer(x)\n\n        (x,) = unpack(x, ps, \"* t d\")\n        x = rearrange(x, \"b f t d -&gt; b t f d\")\n        x, ps = pack([x], \"* f d\")\n\n        if self.use_torch_checkpoint:\n            x = checkpoint(freq_transformer, x, use_reentrant=False)\n        else:\n            x = freq_transformer(x)\n\n        (x,) = unpack(x, ps, \"* f d\")\n\n        if self.skip_connection:\n            store[i] = x\n\n    x = self.final_norm(x)\n\n    if self.use_torch_checkpoint:\n        mask = torch.stack(\n            [checkpoint(fn, x, use_reentrant=False) for fn in self.mask_estimators],\n            dim=1,\n        )\n    else:\n        mask = torch.stack([fn(x) for fn in self.mask_estimators], dim=1)\n    mask = rearrange(mask, \"b n t (f c) -&gt; b n f t c\", c=2)\n\n    if not self.is_mel:\n        return mask\n\n    stft_repr = rearrange(stft_repr, \"b f t c -&gt; b 1 f t c\")\n    # stft_repr may be fp16 but complex32 support is experimental so we upcast it early\n    stft_repr_complex = torch.view_as_complex(stft_repr.to(torch.float32))\n\n    masks_per_band_complex = torch.view_as_complex(mask)\n    masks_per_band_complex = masks_per_band_complex.type(stft_repr_complex.dtype)\n\n    scatter_indices = repeat(\n        self.freq_indices,\n        \"f -&gt; b n f t\",\n        b=batch,\n        n=self.num_stems,\n        t=stft_repr_complex.shape[-1],\n    )\n    stft_repr_expanded_stems = repeat(stft_repr_complex, \"b 1 ... -&gt; b n ...\", n=self.num_stems)\n\n    masks_summed = torch.zeros_like(stft_repr_expanded_stems).scatter_add_(\n        2, scatter_indices, masks_per_band_complex\n    )\n\n    denom = repeat(self.num_bands_per_freq, \"f -&gt; (f r) 1\", r=self.audio_channels)\n    masks_averaged = masks_summed / denom.clamp(min=1e-8)\n\n    return torch.view_as_real(masks_averaged).to(stft_repr.dtype)\n</code></pre>"},{"location":"api/models/#splifft.models.utils","title":"utils","text":"<p>Modules:</p> Name Description <code>attend</code> <code>attend_sage</code> <code>stft</code> <p>Functions:</p> Name Description <code>parse_version</code> <code>log_once</code>"},{"location":"api/models/#splifft.models.utils.parse_version","title":"parse_version","text":"<pre><code>parse_version(v_str: str) -&gt; tuple[int, ...]\n</code></pre> Source code in <code>src/splifft/models/utils/__init__.py</code> <pre><code>def parse_version(v_str: str) -&gt; tuple[int, ...]:\n    # e.g \"2.1.0+cu118\" -&gt; (2, 1, 0)\n    return tuple(map(int, v_str.split(\"+\")[0].split(\".\")))\n</code></pre>"},{"location":"api/models/#splifft.models.utils.log_once","title":"log_once  <code>cached</code>","text":"<pre><code>log_once(\n    logger: Logger, msg: object, *, level: int = DEBUG\n) -&gt; None\n</code></pre> Source code in <code>src/splifft/models/utils/__init__.py</code> <pre><code>@lru_cache(10)\ndef log_once(logger: Logger, msg: object, *, level: int = logging.DEBUG) -&gt; None:\n    logger.log(level, msg)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.attend_sage","title":"attend_sage","text":"<p>Classes:</p> Name Description <code>AttendSage</code> <p>Attributes:</p> Name Type Description <code>logger</code>"},{"location":"api/models/#splifft.models.utils.attend_sage.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.attend_sage.AttendSage","title":"AttendSage","text":"<pre><code>AttendSage(\n    dropout: float = 0.0,\n    flash: bool = False,\n    scale: float | None = None,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Parameters:</p> Name Type Description Default <code>flash</code> <code>bool</code> <p>if True, attempts to use SageAttention or PyTorch SDPA.</p> <code>False</code> <p>Methods:</p> Name Description <code>forward</code> <p>einstein notation</p> <p>Attributes:</p> Name Type Description <code>scale</code> <code>dropout</code> <code>use_sage</code> <code>use_pytorch_sdpa</code> <code>attn_dropout</code> Source code in <code>src/splifft/models/utils/attend_sage.py</code> <pre><code>def __init__(\n    self,\n    dropout: float = 0.0,\n    flash: bool = False,\n    scale: float | None = None,\n):\n    \"\"\"\n    :param flash: if True, attempts to use SageAttention or PyTorch SDPA.\n    \"\"\"\n    super().__init__()\n    self.scale = scale  # for einsum path\n    self.dropout = dropout  # for einsum/SDPA path\n\n    self.use_sage = flash and _has_sage_attention\n    self.use_pytorch_sdpa = False\n    self._sdpa_checked = False\n\n    if flash and not self.use_sage:\n        if not self._sdpa_checked:\n            if parse_version(torch.__version__) &gt;= (2, 0, 0):\n                self.use_pytorch_sdpa = True\n                log_once(\n                    logger,\n                    \"Using PyTorch SDPA backend (FlashAttention-2, Memory-Efficient, or Math).\",\n                )\n            else:\n                log_once(\n                    logger,\n                    \"Flash attention requested but Pytorch &lt; 2.0 and SageAttention not found. Falling back to einsum.\",\n                )\n            self._sdpa_checked = True\n\n    # dropout layer for manual einsum implementation ONLY\n    # SDPA and SageAttention handle dropout differently\n    # (or not at all in Sage's base API)\n    self.attn_dropout = nn.Dropout(dropout)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.attend_sage.AttendSage.scale","title":"scale  <code>instance-attribute</code>","text":"<pre><code>scale = scale\n</code></pre>"},{"location":"api/models/#splifft.models.utils.attend_sage.AttendSage.dropout","title":"dropout  <code>instance-attribute</code>","text":"<pre><code>dropout = dropout\n</code></pre>"},{"location":"api/models/#splifft.models.utils.attend_sage.AttendSage.use_sage","title":"use_sage  <code>instance-attribute</code>","text":"<pre><code>use_sage = flash and _has_sage_attention\n</code></pre>"},{"location":"api/models/#splifft.models.utils.attend_sage.AttendSage.use_pytorch_sdpa","title":"use_pytorch_sdpa  <code>instance-attribute</code>","text":"<pre><code>use_pytorch_sdpa = False\n</code></pre>"},{"location":"api/models/#splifft.models.utils.attend_sage.AttendSage.attn_dropout","title":"attn_dropout  <code>instance-attribute</code>","text":"<pre><code>attn_dropout = Dropout(dropout)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.attend_sage.AttendSage.forward","title":"forward","text":"<pre><code>forward(q: Tensor, k: Tensor, v: Tensor) -&gt; Tensor\n</code></pre> <p>einstein notation</p> <ul> <li>b: batch</li> <li>h: heads</li> <li>n, i, j: sequence length (base sequence length, source, target)</li> <li>d: feature dimension</li> </ul> <p>Input tensors q, k, v expected in shape: (batch, heads, seq_len, dim_head) -&gt; HND layout</p> Source code in <code>src/splifft/models/utils/attend_sage.py</code> <pre><code>def forward(self, q: Tensor, k: Tensor, v: Tensor) -&gt; Tensor:\n    \"\"\"\n    einstein notation\n\n    - b: batch\n    - h: heads\n    - n, i, j: sequence length (base sequence length, source, target)\n    - d: feature dimension\n\n    Input tensors q, k, v expected in shape: (batch, heads, seq_len, dim_head) -&gt; HND layout\n    \"\"\"\n    _q_len, _k_len, _device = q.shape[-2], k.shape[-2], q.device\n\n    # priority 1: SageAttention\n    if self.use_sage:\n        # assumes q, k, v are FP16/BF16 (handled by autocast upstream)\n        # assumes scale is handled internally by sageattn\n        # assumes dropout is NOT handled by sageattn kernel\n        # is_causal=False based on how Attend is called in mel_band_roformer\n        out = sageattn(q, k, v, tensor_layout=\"HND\", is_causal=False)  # type: ignore\n        return out  # type: ignore\n        try:\n            out = sageattn(q, k, v, tensor_layout=\"HND\", is_causal=False)\n            return out\n        except Exception as e:\n            logger.error(f\"SageAttention failed with error: {e}. Falling back.\")\n            self.use_sage = False\n            if not self._sdpa_checked:\n                if parse_version(torch.__version__) &gt;= (2, 0, 0):\n                    self.use_pytorch_sdpa = True\n                    log_once(logger, \"falling back to PyTorch SDPA\")\n                else:\n                    log_once(logger, \"falling back to einsum.\")\n\n                self._sdpa_checked = True\n\n    # priority 2: PyTorch SDPA\n    if self.use_pytorch_sdpa:\n        # it handles scaling and dropout internally.\n        try:\n            with sdpa_kernel(\n                [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION, SDPBackend.MATH]\n            ):\n                out = F.scaled_dot_product_attention(\n                    q,\n                    k,\n                    v,\n                    attn_mask=None,  # assuming no explicit mask needed here\n                    dropout_p=self.dropout if self.training else 0.0,\n                    is_causal=False,  # assuming not needed based on usage context\n                )\n            return out\n        except Exception as e:\n            log_once(\n                logger,\n                f\"pytorch SDPA failed with error: {e}. falling back to einsum.\",\n                level=logging.ERROR,\n            )\n            self.use_pytorch_sdpa = False\n\n    scale = self.scale or q.shape[-1] ** -0.5\n\n    # similarity\n    sim = einsum(\"b h i d, b h j d -&gt; b h i j\", q, k) * scale\n\n    # attention\n    attn = sim.softmax(dim=-1)\n    attn = self.attn_dropout(attn)  # ONLY in einsum path\n\n    # aggregate values\n    out = einsum(\"b h i j, b h j d -&gt; b h i d\", attn, v)\n\n    return out\n</code></pre>"},{"location":"api/models/#splifft.models.utils.attend","title":"attend","text":"<p>Classes:</p> Name Description <code>Attend</code> <p>Attributes:</p> Name Type Description <code>logger</code>"},{"location":"api/models/#splifft.models.utils.attend.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.attend.Attend","title":"Attend","text":"<pre><code>Attend(\n    dropout: float = 0.0,\n    flash: bool = False,\n    scale: float | None = None,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Methods:</p> Name Description <code>flash_attn</code> <code>forward</code> <p>einstein notation</p> <p>Attributes:</p> Name Type Description <code>scale</code> <code>dropout</code> <code>attn_dropout</code> <code>flash</code> <code>cpu_backends</code> <code>cuda_backends</code> <code>list[_SDPBackend] | None</code> Source code in <code>src/splifft/models/utils/attend.py</code> <pre><code>def __init__(\n    self, dropout: float = 0.0, flash: bool = False, scale: float | None = None\n) -&gt; None:\n    super().__init__()\n    self.scale = scale\n    self.dropout = dropout\n    self.attn_dropout = nn.Dropout(dropout)\n\n    self.flash = flash\n    assert not (flash and parse_version(torch.__version__) &lt; (2, 0, 0)), (\n        \"expected pytorch &gt;= 2.0.0 to use flash attention\"\n    )\n\n    # determine efficient attention configs for cuda and cpu\n    self.cpu_backends = [\n        SDPBackend.FLASH_ATTENTION,\n        SDPBackend.EFFICIENT_ATTENTION,\n        SDPBackend.MATH,\n    ]\n    self.cuda_backends: list[_SDPBackend] | None = None\n\n    if not torch.cuda.is_available() or not flash:\n        return\n\n    device_properties = torch.cuda.get_device_properties(torch.device(\"cuda\"))\n    device_version = parse_version(f\"{device_properties.major}.{device_properties.minor}\")\n\n    if device_version &gt;= (8, 0):\n        if os.name == \"nt\":\n            cuda_backends = [SDPBackend.EFFICIENT_ATTENTION, SDPBackend.MATH]\n            log_once(logger, f\"windows detected, using {cuda_backends=}\")\n        else:\n            cuda_backends = [SDPBackend.FLASH_ATTENTION]\n            log_once(logger, f\"gpu compute capability &gt;= 8.0, using {cuda_backends=}\")\n    else:\n        cuda_backends = [SDPBackend.EFFICIENT_ATTENTION, SDPBackend.MATH]\n        log_once(logger, f\"gpu compute capability &lt; 8.0, using {cuda_backends=}\")\n\n    self.cuda_backends = cuda_backends\n</code></pre>"},{"location":"api/models/#splifft.models.utils.attend.Attend.scale","title":"scale  <code>instance-attribute</code>","text":"<pre><code>scale = scale\n</code></pre>"},{"location":"api/models/#splifft.models.utils.attend.Attend.dropout","title":"dropout  <code>instance-attribute</code>","text":"<pre><code>dropout = dropout\n</code></pre>"},{"location":"api/models/#splifft.models.utils.attend.Attend.attn_dropout","title":"attn_dropout  <code>instance-attribute</code>","text":"<pre><code>attn_dropout = Dropout(dropout)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.attend.Attend.flash","title":"flash  <code>instance-attribute</code>","text":"<pre><code>flash = flash\n</code></pre>"},{"location":"api/models/#splifft.models.utils.attend.Attend.cpu_backends","title":"cpu_backends  <code>instance-attribute</code>","text":"<pre><code>cpu_backends = [FLASH_ATTENTION, EFFICIENT_ATTENTION, MATH]\n</code></pre>"},{"location":"api/models/#splifft.models.utils.attend.Attend.cuda_backends","title":"cuda_backends  <code>instance-attribute</code>","text":"<pre><code>cuda_backends: list[_SDPBackend] | None = cuda_backends\n</code></pre>"},{"location":"api/models/#splifft.models.utils.attend.Attend.flash_attn","title":"flash_attn","text":"<pre><code>flash_attn(q: Tensor, k: Tensor, v: Tensor) -&gt; Tensor\n</code></pre> Source code in <code>src/splifft/models/utils/attend.py</code> <pre><code>def flash_attn(self, q: Tensor, k: Tensor, v: Tensor) -&gt; Tensor:\n    _, _heads, _q_len, _, _k_len, is_cuda, _device = (\n        *q.shape,\n        k.shape[-2],\n        q.is_cuda,\n        q.device,\n    )  # type: ignore\n\n    if self.scale is not None:\n        default_scale = q.shape[-1] ** -0.5\n        q = q * (self.scale / default_scale)\n\n    backends = self.cuda_backends if is_cuda else self.cpu_backends\n    # pytorch 2.0 flash attn: q, k, v, mask, dropout, softmax_scale\n    with sdpa_kernel(backends=backends):  # type: ignore\n        out = F.scaled_dot_product_attention(\n            q, k, v, dropout_p=self.dropout if self.training else 0.0\n        )\n\n    return out\n</code></pre>"},{"location":"api/models/#splifft.models.utils.attend.Attend.forward","title":"forward","text":"<pre><code>forward(q: Tensor, k: Tensor, v: Tensor) -&gt; Tensor\n</code></pre> <p>einstein notation</p> <ul> <li>b: batch</li> <li>h: heads</li> <li>n, i, j: sequence length (base sequence length, source, target)</li> <li>d: feature dimension</li> </ul> Source code in <code>src/splifft/models/utils/attend.py</code> <pre><code>def forward(self, q: Tensor, k: Tensor, v: Tensor) -&gt; Tensor:\n    \"\"\"\n    einstein notation\n\n    - b: batch\n    - h: heads\n    - n, i, j: sequence length (base sequence length, source, target)\n    - d: feature dimension\n    \"\"\"\n    _q_len, _k_len, _device = q.shape[-2], k.shape[-2], q.device\n\n    scale = self.scale or q.shape[-1] ** -0.5\n\n    if self.flash:\n        return self.flash_attn(q, k, v)\n\n    # similarity\n    sim = einsum(\"b h i d, b h j d -&gt; b h i j\", q, k) * scale\n\n    # attention\n    attn = sim.softmax(dim=-1)\n    attn = self.attn_dropout(attn)\n\n    # aggregate values\n    out = einsum(\"b h i j, b h j d -&gt; b h i d\", attn, v)\n\n    return out\n</code></pre>"},{"location":"api/models/#splifft.models.utils.stft","title":"stft","text":"<p>Classes:</p> Name Description <code>Stft</code> <p>A custom STFT implementation using 1D convolutions to ensure compatibility with CoreML.</p> <code>IStft</code> <p>A simple wrapper around torch.istft with a hacky workaround for MPS.</p>"},{"location":"api/models/#splifft.models.utils.stft.Stft","title":"Stft","text":"<pre><code>Stft(\n    n_fft: int,\n    hop_length: int,\n    win_length: int,\n    window_fn: Callable[[int], Tensor],\n    conv_dtype: dtype | None,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>A custom STFT implementation using 1D convolutions to ensure compatibility with CoreML.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>n_fft</code> <code>hop_length</code> <code>win_length</code> <code>conv_dtype</code> Source code in <code>src/splifft/models/utils/stft.py</code> <pre><code>def __init__(\n    self,\n    n_fft: int,\n    hop_length: int,\n    win_length: int,\n    window_fn: Callable[[int], Tensor],\n    conv_dtype: torch.dtype | None,\n):\n    super().__init__()\n    self.n_fft = n_fft\n    self.hop_length = hop_length\n    self.win_length = win_length\n    self.conv_dtype = conv_dtype\n\n    window = window_fn(self.win_length)\n\n    dft_mat = torch.fft.fft(torch.eye(self.n_fft, device=window.device))\n    dft_mat_T = dft_mat.T\n\n    real_kernels = dft_mat_T.real[\n        : self.win_length, : (self.n_fft // 2 + 1)\n    ] * window.unsqueeze(-1)\n    imag_kernels = dft_mat_T.imag[\n        : self.win_length, : (self.n_fft // 2 + 1)\n    ] * window.unsqueeze(-1)\n\n    # (out_channels, in_channels, kernel_size)\n    self.register_buffer(\"real_conv_weight\", real_kernels.T.unsqueeze(1).to(self.conv_dtype))\n    self.register_buffer(\"imag_conv_weight\", imag_kernels.T.unsqueeze(1).to(self.conv_dtype))\n</code></pre>"},{"location":"api/models/#splifft.models.utils.stft.Stft.n_fft","title":"n_fft  <code>instance-attribute</code>","text":"<pre><code>n_fft = n_fft\n</code></pre>"},{"location":"api/models/#splifft.models.utils.stft.Stft.hop_length","title":"hop_length  <code>instance-attribute</code>","text":"<pre><code>hop_length = hop_length\n</code></pre>"},{"location":"api/models/#splifft.models.utils.stft.Stft.win_length","title":"win_length  <code>instance-attribute</code>","text":"<pre><code>win_length = win_length\n</code></pre>"},{"location":"api/models/#splifft.models.utils.stft.Stft.conv_dtype","title":"conv_dtype  <code>instance-attribute</code>","text":"<pre><code>conv_dtype = conv_dtype\n</code></pre>"},{"location":"api/models/#splifft.models.utils.stft.Stft.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; ComplexSpectrogram\n</code></pre> Source code in <code>src/splifft/models/utils/stft.py</code> <pre><code>def forward(self, x: Tensor) -&gt; t.ComplexSpectrogram:\n    b, s, t = x.shape\n    x = x.reshape(b * s, 1, t).to(self.conv_dtype)\n\n    padding = self.n_fft // 2\n    x = F.pad(x, (padding, padding), \"reflect\")\n\n    real_part = F.conv1d(x, self.real_conv_weight, stride=self.hop_length)  # type: ignore\n    imag_part = F.conv1d(x, self.imag_conv_weight, stride=self.hop_length)  # type: ignore\n    spec = torch.stack((real_part, imag_part), dim=-1)  # (b*s, f, t_frames, c=2)\n\n    _bs, f, t_frames, c = spec.shape\n    spec = spec.view(b, s, f, t_frames, c)\n\n    return spec  # type: ignore\n</code></pre>"},{"location":"api/models/#splifft.models.utils.stft.IStft","title":"IStft","text":"<pre><code>IStft(\n    n_fft: int,\n    hop_length: int,\n    win_length: int,\n    window_fn: Callable[[int], Tensor] = hann_window,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>A simple wrapper around torch.istft with a hacky workaround for MPS.</p> <p>TODO: implement a proper workaround.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>n_fft</code> <code>hop_length</code> <code>win_length</code> <code>window</code> Source code in <code>src/splifft/models/utils/stft.py</code> <pre><code>def __init__(\n    self,\n    n_fft: int,\n    hop_length: int,\n    win_length: int,\n    window_fn: Callable[[int], Tensor] = torch.hann_window,\n):\n    super().__init__()\n    self.n_fft = n_fft\n    self.hop_length = hop_length\n    self.win_length = win_length\n    self.window = window_fn(self.win_length)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.stft.IStft.n_fft","title":"n_fft  <code>instance-attribute</code>","text":"<pre><code>n_fft = n_fft\n</code></pre>"},{"location":"api/models/#splifft.models.utils.stft.IStft.hop_length","title":"hop_length  <code>instance-attribute</code>","text":"<pre><code>hop_length = hop_length\n</code></pre>"},{"location":"api/models/#splifft.models.utils.stft.IStft.win_length","title":"win_length  <code>instance-attribute</code>","text":"<pre><code>win_length = win_length\n</code></pre>"},{"location":"api/models/#splifft.models.utils.stft.IStft.window","title":"window  <code>instance-attribute</code>","text":"<pre><code>window = window_fn(win_length)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.stft.IStft.forward","title":"forward","text":"<pre><code>forward(\n    spec: ComplexSpectrogram, length: int | None = None\n) -&gt; RawAudioTensor | NormalizedAudioTensor\n</code></pre> Source code in <code>src/splifft/models/utils/stft.py</code> <pre><code>def forward(\n    self, spec: t.ComplexSpectrogram, length: int | None = None\n) -&gt; t.RawAudioTensor | t.NormalizedAudioTensor:\n    device = spec.device\n    is_mps = device.type == \"mps\"\n    window = self.window.to(device)\n    # see https://github.com/lucidrains/BS-RoFormer/issues/47\n    # this would introduce a breaking change.\n    # spec = spec.index_fill(1, torch.tensor(0, device=spec.device), 0.)  # type: ignore\n    spec_complex = torch.view_as_complex(spec)\n\n    try:\n        audio = torch.istft(\n            spec_complex,\n            n_fft=self.n_fft,\n            hop_length=self.hop_length,\n            win_length=self.win_length,\n            window=window,\n            return_complex=False,\n            length=length,\n        )\n    except RuntimeError:\n        audio = torch.istft(\n            spec_complex.cpu() if is_mps else spec_complex,\n            n_fft=self.n_fft,\n            hop_length=self.hop_length,\n            win_length=self.win_length,\n            window=window.cpu() if is_mps else window,\n            return_complex=False,\n            length=length,\n        ).to(device)\n\n    return audio  # type: ignore\n</code></pre>"},{"location":"api/training/","title":"Training","text":""},{"location":"api/training/#splifft.training","title":"training","text":"<p>High level orchestrator for model training</p> <p>Warning</p> <p>This module is incomplete. they only contain annotations for future use.</p> <p>Attributes:</p> Name Type Description <code>Epoch</code> <code>TypeAlias</code> <p>The number of times the model has seen the entire training dataset.</p> <code>TrainingBatchSize</code> <code>TypeAlias</code> <p>Number of training examples (audio chunks) processed before a weight update.</p> <code>GradientAccumulationSteps</code> <code>TypeAlias</code> <p>Number of batches to process before performing a weight update.</p> <code>OptimizerName</code> <code>TypeAlias</code> <p>Algorithm used to update the model weights to minimize the loss function.</p> <code>LearningRateSchedulerName</code> <code>TypeAlias</code> <p>Algorithm used to adjust the learning rate during training.</p> <code>UseAutomaticMixedPrecision</code> <code>TypeAlias</code> <p>Whether to use automatic mixed precision (AMP) during training.</p> <code>UseLoRA</code> <code>TypeAlias</code> <p>Whether to use Low-Rank Adaptation for efficient fine-tuning.</p>"},{"location":"api/training/#splifft.training.Epoch","title":"Epoch  <code>module-attribute</code>","text":"<pre><code>Epoch: TypeAlias = int\n</code></pre> <p>The number of times the model has seen the entire training dataset.</p>"},{"location":"api/training/#splifft.training.TrainingBatchSize","title":"TrainingBatchSize  <code>module-attribute</code>","text":"<pre><code>TrainingBatchSize: TypeAlias = Annotated[int, Gt(0)]\n</code></pre> <p>Number of training examples (audio chunks) processed before a weight update.</p> <p>Larger batches may offer more stable gradients but require more memory.</p>"},{"location":"api/training/#splifft.training.GradientAccumulationSteps","title":"GradientAccumulationSteps  <code>module-attribute</code>","text":"<pre><code>GradientAccumulationSteps: TypeAlias = Annotated[int, Gt(0)]\n</code></pre> <p>Number of batches to process before performing a weight update.</p> <p>This simulates a larger batch size without increasing memory, e.g., a batch size of 4 with 8 accumulation steps has an effective batch size of 32.</p>"},{"location":"api/training/#splifft.training.OptimizerName","title":"OptimizerName  <code>module-attribute</code>","text":"<pre><code>OptimizerName: TypeAlias = str\n</code></pre> <p>Algorithm used to update the model weights to minimize the loss function.</p>"},{"location":"api/training/#splifft.training.LearningRateSchedulerName","title":"LearningRateSchedulerName  <code>module-attribute</code>","text":"<pre><code>LearningRateSchedulerName: TypeAlias = str\n</code></pre> <p>Algorithm used to adjust the learning rate during training.</p> <p>e.g. <code>ReduceLROnPlateau</code> can reduce the learning rate when a metric stops improving.</p>"},{"location":"api/training/#splifft.training.UseAutomaticMixedPrecision","title":"UseAutomaticMixedPrecision  <code>module-attribute</code>","text":"<pre><code>UseAutomaticMixedPrecision: TypeAlias = bool\n</code></pre> <p>Whether to use automatic mixed precision (AMP) during training.</p>"},{"location":"api/training/#splifft.training.UseLoRA","title":"UseLoRA  <code>module-attribute</code>","text":"<pre><code>UseLoRA: TypeAlias = bool\n</code></pre> <p>Whether to use Low-Rank Adaptation for efficient fine-tuning.</p> <p>This freezes pre-trained weights and injects smaller, trainable low-rank matrices, dramatically reducing the number of trainable parameters.</p>"},{"location":"api/types/","title":"Types","text":""},{"location":"api/types/#splifft.types","title":"types","text":"<p>Types for documentation and data validation (for use in pydantic).</p> <p>They provide semantic meaning only and we additionally use <code>NewType</code> for strong semantic distinction to avoid mixing up different kinds of tensors.</p> <p>Note that no code implementations shall be placed here.</p> <p>Attributes:</p> Name Type Description <code>StrPath</code> <code>TypeAlias</code> <code>BytesPath</code> <code>TypeAlias</code> <code>Gt0</code> <code>TypeAlias</code> <code>Ge0</code> <code>TypeAlias</code> <code>ModelType</code> <code>TypeAlias</code> <p>The type of the model, e.g. <code>bs_roformer</code>, <code>demucs</code></p> <code>ModelInputType</code> <code>TypeAlias</code> <code>ModelOutputType</code> <code>TypeAlias</code> <code>ChunkSize</code> <code>TypeAlias</code> <p>The length of an audio segment, in samples, processed by the model at one time.</p> <code>HopSize</code> <code>TypeAlias</code> <p>The step size, in samples, between the start of consecutive chunks.</p> <code>Dropout</code> <code>TypeAlias</code> <code>ModelOutputStemName</code> <code>TypeAlias</code> <p>The output stem name, e.g. <code>vocals</code>, <code>drums</code>, <code>bass</code>, etc.</p> <code>Samples</code> <code>TypeAlias</code> <p>Number of samples in the audio signal.</p> <code>SampleRate</code> <code>TypeAlias</code> <p>The number of samples of audio recorded per second (hertz).</p> <code>Channels</code> <code>TypeAlias</code> <p>Number of audio streams.</p> <code>FileFormat</code> <code>TypeAlias</code> <code>BitRate</code> <code>TypeAlias</code> <p>Number of bits of information in each sample.</p> <code>RawAudioTensor</code> <p>Time domain tensor of audio samples.</p> <code>NormalizedAudioTensor</code> <p>A mixture tensor that has been normalized using on-the-fly statistics.</p> <code>ComplexSpectrogram</code> <p>A complex-valued representation of audio's frequency content over time via the STFT.</p> <code>HybridModelInput</code> <code>TypeAlias</code> <p>Input for hybrid models that require both spectrogram and waveform.</p> <code>WindowShape</code> <code>TypeAlias</code> <p>The shape of the window function applied to each chunk before computing the STFT.</p> <code>FftSize</code> <code>TypeAlias</code> <p>The number of frequency bins in the STFT, controlling the frequency resolution.</p> <code>Bands</code> <code>TypeAlias</code> <p>Groups of adjacent frequency bins in the spectrogram.</p> <code>BatchSize</code> <code>TypeAlias</code> <p>The number of chunks processed simultaneously by the GPU.</p> <code>PaddingMode</code> <code>TypeAlias</code> <p>The method used to pad the audio before chunking, crucial for handling the edges of the audio signal.</p> <code>ChunkDuration</code> <code>TypeAlias</code> <p>The length of an audio segment, in seconds, processed by the model at one time.</p> <code>OverlapRatio</code> <code>TypeAlias</code> <p>The fraction of a chunk that overlaps with the next one.</p> <code>Padding</code> <code>TypeAlias</code> <p>Samples to add to the beginning and end of each chunk.</p> <code>PaddedChunkedAudioTensor</code> <p>A batch of audio chunks from a padded source.</p> <code>NumModelStems</code> <code>TypeAlias</code> <p>The number of stems the model outputs. This should be the length of [splifft.models.ModelParamsLike.output_stem_names].</p> <code>SeparatedSpectrogramTensor</code> <p>A batch of separated spectrograms.</p> <code>SeparatedChunkedTensor</code> <p>A batch of separated audio chunks from the model.</p> <code>WindowTensor</code> <p>A 1D tensor representing a window function.</p> <code>RawSeparatedTensor</code> <p>The final, stitched, raw-domain separated audio.</p> <code>PreprocessFn</code> <code>TypeAlias</code> <code>PostprocessFn</code> <code>TypeAlias</code> <code>Identifier</code> <code>TypeAlias</code> <p><code>{{architecture}}-{{first_author}}-{{unique_name_short}}</code>, use underscore if it has spaces</p> <code>Instrument</code> <code>TypeAlias</code> <code>Metric</code> <code>TypeAlias</code> <code>Sdr</code> <code>TypeAlias</code> <p>Signal-to-Distortion Ratio (decibels). Higher is better.</p> <code>SiSdr</code> <code>TypeAlias</code> <p>Scale-Invariant SDR (SI-SDR) is invariant to scaling errors (decibels). Higher is better.</p> <code>L1Norm</code> <code>TypeAlias</code> <p>L1 norm (mean absolute error) between two signals (dimensionless). Lower is better.</p> <code>DbDifferenceMel</code> <code>TypeAlias</code> <p>Difference in the dB-scaled mel spectrogram.</p> <code>Bleedless</code> <code>TypeAlias</code> <p>A metric to quantify the amount of \"bleeding\" from other sources. Higher is better.</p> <code>Fullness</code> <code>TypeAlias</code> <p>A metric to quantify how much of the original source is missing. Higher is better.</p>"},{"location":"api/types/#splifft.types.StrPath","title":"StrPath  <code>module-attribute</code>","text":"<pre><code>StrPath: TypeAlias = str | PathLike[str]\n</code></pre>"},{"location":"api/types/#splifft.types.BytesPath","title":"BytesPath  <code>module-attribute</code>","text":"<pre><code>BytesPath: TypeAlias = bytes | PathLike[bytes]\n</code></pre>"},{"location":"api/types/#splifft.types.Gt0","title":"Gt0  <code>module-attribute</code>","text":"<pre><code>Gt0: TypeAlias = Annotated[_T, Gt(0)]\n</code></pre>"},{"location":"api/types/#splifft.types.Ge0","title":"Ge0  <code>module-attribute</code>","text":"<pre><code>Ge0: TypeAlias = Annotated[_T, Ge(0)]\n</code></pre>"},{"location":"api/types/#splifft.types.ModelType","title":"ModelType  <code>module-attribute</code>","text":"<pre><code>ModelType: TypeAlias = str\n</code></pre> <p>The type of the model, e.g. <code>bs_roformer</code>, <code>demucs</code></p>"},{"location":"api/types/#splifft.types.ModelInputType","title":"ModelInputType  <code>module-attribute</code>","text":"<pre><code>ModelInputType: TypeAlias = Literal[\n    \"waveform\", \"spectrogram\", \"waveform_and_spectrogram\"\n]\n</code></pre>"},{"location":"api/types/#splifft.types.ModelOutputType","title":"ModelOutputType  <code>module-attribute</code>","text":"<pre><code>ModelOutputType: TypeAlias = Literal[\n    \"waveform\", \"spectrogram_mask\", \"spectrogram\"\n]\n</code></pre>"},{"location":"api/types/#splifft.types.ChunkSize","title":"ChunkSize  <code>module-attribute</code>","text":"<pre><code>ChunkSize: TypeAlias = Gt0[int]\n</code></pre> <p>The length of an audio segment, in samples, processed by the model at one time.</p> <p>A full audio track is often too long to fit into GPU, instead we process it in fixed-size chunks. A larger chunk size may allow the model to capture more temporal context at the cost of increased memory usage.</p>"},{"location":"api/types/#splifft.types.HopSize","title":"HopSize  <code>module-attribute</code>","text":"<pre><code>HopSize: TypeAlias = Gt0[int]\n</code></pre> <p>The step size, in samples, between the start of consecutive chunks.</p> <p>To avoid artifacts at the edges of chunks, we process them with overlap. The hop size is the distance we \"slide\" the chunking window forward. <code>ChunkSize &lt; HopSize</code> implies overlap and the overlap amount is <code>ChunkSize - HopSize</code>.</p>"},{"location":"api/types/#splifft.types.Dropout","title":"Dropout  <code>module-attribute</code>","text":"<pre><code>Dropout: TypeAlias = Annotated[float, Ge(0.0), Le(1.0)]\n</code></pre>"},{"location":"api/types/#splifft.types.ModelOutputStemName","title":"ModelOutputStemName  <code>module-attribute</code>","text":"<pre><code>ModelOutputStemName: TypeAlias = Annotated[str, MinLen(1)]\n</code></pre> <p>The output stem name, e.g. <code>vocals</code>, <code>drums</code>, <code>bass</code>, etc.</p>"},{"location":"api/types/#splifft.types.Samples","title":"Samples  <code>module-attribute</code>","text":"<pre><code>Samples: TypeAlias = Gt0[int]\n</code></pre> <p>Number of samples in the audio signal.</p>"},{"location":"api/types/#splifft.types.SampleRate","title":"SampleRate  <code>module-attribute</code>","text":"<pre><code>SampleRate: TypeAlias = Gt0[int]\n</code></pre> <p>The number of samples of audio recorded per second (hertz).</p> <p>See concepts for more details.</p>"},{"location":"api/types/#splifft.types.Channels","title":"Channels  <code>module-attribute</code>","text":"<pre><code>Channels: TypeAlias = Gt0[int]\n</code></pre> <p>Number of audio streams.</p> <ul> <li>1: Mono audio</li> <li>2: Stereo (left and right). Models are usually trained on stereo audio.</li> </ul>"},{"location":"api/types/#splifft.types.FileFormat","title":"FileFormat  <code>module-attribute</code>","text":"<pre><code>FileFormat: TypeAlias = Literal['flac', 'wav', 'ogg']\n</code></pre>"},{"location":"api/types/#splifft.types.BitRate","title":"BitRate  <code>module-attribute</code>","text":"<pre><code>BitRate: TypeAlias = Literal[8, 16, 24, 32, 64]\n</code></pre> <p>Number of bits of information in each sample.</p> <p>It determines the dynamic range of the audio signal: the difference between the quietest and loudest possible sounds.</p> <ul> <li>16-bit: Standard for CD audio: ~96 dB dynamic range.</li> <li>24-bit: Common in professional audio, allowing for more headroom during mixing</li> <li>32-bit float: Standard in digital audio workstations (DAWs) and deep learning models.     The amplitude is represented by a floating-point number, which prevents clipping (distortion     from exceeding the maximum value). This library primarily works with fp32 tensors.</li> </ul>"},{"location":"api/types/#splifft.types.RawAudioTensor","title":"RawAudioTensor  <code>module-attribute</code>","text":"<pre><code>RawAudioTensor = NewType('RawAudioTensor', Tensor)\n</code></pre> <p>Time domain tensor of audio samples. Shape (channels, samples)</p>"},{"location":"api/types/#splifft.types.NormalizedAudioTensor","title":"NormalizedAudioTensor  <code>module-attribute</code>","text":"<pre><code>NormalizedAudioTensor = NewType(\n    \"NormalizedAudioTensor\", Tensor\n)\n</code></pre> <p>A mixture tensor that has been normalized using on-the-fly statistics. Shape (channels, samples)</p>"},{"location":"api/types/#splifft.types.ComplexSpectrogram","title":"ComplexSpectrogram  <code>module-attribute</code>","text":"<pre><code>ComplexSpectrogram = NewType('ComplexSpectrogram', Tensor)\n</code></pre> <p>A complex-valued representation of audio's frequency content over time via the STFT.</p> <p>Shape (channels, frequency bins, time frames, 2)</p> <p>See concepts for more details.</p>"},{"location":"api/types/#splifft.types.HybridModelInput","title":"HybridModelInput  <code>module-attribute</code>","text":"<pre><code>HybridModelInput: TypeAlias = tuple[\n    ComplexSpectrogram,\n    RawAudioTensor | NormalizedAudioTensor,\n]\n</code></pre> <p>Input for hybrid models that require both spectrogram and waveform.</p>"},{"location":"api/types/#splifft.types.WindowShape","title":"WindowShape  <code>module-attribute</code>","text":"<pre><code>WindowShape: TypeAlias = Literal[\n    \"hann\", \"hamming\", \"linear_fade\"\n]\n</code></pre> <p>The shape of the window function applied to each chunk before computing the STFT.</p>"},{"location":"api/types/#splifft.types.FftSize","title":"FftSize  <code>module-attribute</code>","text":"<pre><code>FftSize: TypeAlias = Gt0[int]\n</code></pre> <p>The number of frequency bins in the STFT, controlling the frequency resolution.</p>"},{"location":"api/types/#splifft.types.Bands","title":"Bands  <code>module-attribute</code>","text":"<pre><code>Bands: TypeAlias = Tensor\n</code></pre> <p>Groups of adjacent frequency bins in the spectrogram.</p>"},{"location":"api/types/#splifft.types.BatchSize","title":"BatchSize  <code>module-attribute</code>","text":"<pre><code>BatchSize: TypeAlias = Gt0[int]\n</code></pre> <p>The number of chunks processed simultaneously by the GPU.</p> <p>Increasing the batch size can improve GPU utilisation and speed up training, but it requires more memory.</p>"},{"location":"api/types/#splifft.types.PaddingMode","title":"PaddingMode  <code>module-attribute</code>","text":"<pre><code>PaddingMode: TypeAlias = Literal[\n    \"reflect\", \"constant\", \"replicate\"\n]\n</code></pre> <p>The method used to pad the audio before chunking, crucial for handling the edges of the audio signal.</p> <ul> <li><code>reflect</code>: Pads the signal by reflecting the audio at the boundary. This creates a smooth   continuation and often yields the best results for music.</li> <li><code>constant</code>: Pads with zeros. Simpler, but can introduce silence at the edges.</li> <li><code>replicate</code>: Repeats the last sample at the edge.</li> </ul>"},{"location":"api/types/#splifft.types.ChunkDuration","title":"ChunkDuration  <code>module-attribute</code>","text":"<pre><code>ChunkDuration: TypeAlias = Gt0[float]\n</code></pre> <p>The length of an audio segment, in seconds, processed by the model at one time.</p> <p>Equivalent to chunk size divided by the sample rate.</p>"},{"location":"api/types/#splifft.types.OverlapRatio","title":"OverlapRatio  <code>module-attribute</code>","text":"<pre><code>OverlapRatio: TypeAlias = Annotated[float, Ge(0), Lt(1)]\n</code></pre> <p>The fraction of a chunk that overlaps with the next one.</p> <p>The relationship with hop size is: $$ \\text{hop_size} = \\text{chunk_size} \\cdot (1 - \\text{overlap_ratio}) $$</p> <ul> <li>A ratio of <code>0.0</code> means no overlap (hop_size = chunk_size).</li> <li>A ratio of <code>0.5</code> means 50% overlap (hop_size = chunk_size / 2).</li> <li>A higher overlap ratio increases computational cost as more chunks are processed, but it can lead   to smoother results by averaging more predictions for each time frame.</li> </ul>"},{"location":"api/types/#splifft.types.Padding","title":"Padding  <code>module-attribute</code>","text":"<pre><code>Padding: TypeAlias = Gt0[int]\n</code></pre> <p>Samples to add to the beginning and end of each chunk.</p> <ul> <li>To ensure that the very beginning and end of a track can be centerd within a chunk, we often may   add \"reflection padding\" or \"zero padding\" before chunking.</li> <li>To ensure that the last chunk is full-size, we may pad the audio so its length is a multiple of   the hop size.</li> </ul>"},{"location":"api/types/#splifft.types.PaddedChunkedAudioTensor","title":"PaddedChunkedAudioTensor  <code>module-attribute</code>","text":"<pre><code>PaddedChunkedAudioTensor = NewType(\n    \"PaddedChunkedAudioTensor\", Tensor\n)\n</code></pre> <p>A batch of audio chunks from a padded source. Shape (batch size, channels, chunk size)</p>"},{"location":"api/types/#splifft.types.NumModelStems","title":"NumModelStems  <code>module-attribute</code>","text":"<pre><code>NumModelStems: TypeAlias = Gt0[int]\n</code></pre> <p>The number of stems the model outputs. This should be the length of [splifft.models.ModelParamsLike.output_stem_names].</p>"},{"location":"api/types/#splifft.types.SeparatedSpectrogramTensor","title":"SeparatedSpectrogramTensor  <code>module-attribute</code>","text":"<pre><code>SeparatedSpectrogramTensor = NewType(\n    \"SeparatedSpectrogramTensor\", Tensor\n)\n</code></pre> <p>A batch of separated spectrograms. Shape (b, n, f*s, t, c=2)</p>"},{"location":"api/types/#splifft.types.SeparatedChunkedTensor","title":"SeparatedChunkedTensor  <code>module-attribute</code>","text":"<pre><code>SeparatedChunkedTensor = NewType(\n    \"SeparatedChunkedTensor\", Tensor\n)\n</code></pre> <p>A batch of separated audio chunks from the model. Shape (batch size, number of stems, channels, chunk size)</p>"},{"location":"api/types/#splifft.types.WindowTensor","title":"WindowTensor  <code>module-attribute</code>","text":"<pre><code>WindowTensor = NewType('WindowTensor', Tensor)\n</code></pre> <p>A 1D tensor representing a window function. Shape (chunk size)</p>"},{"location":"api/types/#splifft.types.RawSeparatedTensor","title":"RawSeparatedTensor  <code>module-attribute</code>","text":"<pre><code>RawSeparatedTensor = NewType('RawSeparatedTensor', Tensor)\n</code></pre> <p>The final, stitched, raw-domain separated audio. Shape (number of stems, channels, samples)</p>"},{"location":"api/types/#splifft.types.PreprocessFn","title":"PreprocessFn  <code>module-attribute</code>","text":"<pre><code>PreprocessFn: TypeAlias = Callable[\n    [RawAudioTensor | NormalizedAudioTensor],\n    tuple[Tensor, ...],\n]\n</code></pre>"},{"location":"api/types/#splifft.types.PostprocessFn","title":"PostprocessFn  <code>module-attribute</code>","text":"<pre><code>PostprocessFn: TypeAlias = Callable[\n    ..., SeparatedChunkedTensor\n]\n</code></pre>"},{"location":"api/types/#splifft.types.Identifier","title":"Identifier  <code>module-attribute</code>","text":"<pre><code>Identifier: TypeAlias = LowerCase[str]\n</code></pre> <p><code>{{architecture}}-{{first_author}}-{{unique_name_short}}</code>, use underscore if it has spaces</p>"},{"location":"api/types/#splifft.types.Instrument","title":"Instrument  <code>module-attribute</code>","text":"<pre><code>Instrument: TypeAlias = Literal[\n    \"instrum\",\n    \"vocals\",\n    \"drums\",\n    \"bass\",\n    \"other\",\n    \"piano\",\n    \"lead_vocals\",\n    \"back_vocals\",\n    \"guitar\",\n    \"vocals1\",\n    \"vocals2\",\n    \"strings\",\n    \"wind\",\n    \"music\",\n    \"sfx\",\n    \"speech\",\n    \"restored\",\n    \"back\",\n    \"lead\",\n    \"back-instrum\",\n    \"kick\",\n    \"snare\",\n    \"toms\",\n    \"hh\",\n    \"cymbals\",\n    \"hh-cymbals\",\n    \"male\",\n    \"female\",\n    \"violin\",\n    \"dry\",\n    \"reverb\",\n    \"clean\",\n    \"crowd\",\n    \"denoised\",\n    \"noise\",\n    \"vocals_dry\",\n    \"dereverb\",\n]\n</code></pre>"},{"location":"api/types/#splifft.types.Metric","title":"Metric  <code>module-attribute</code>","text":"<pre><code>Metric: TypeAlias = Literal[\n    \"sdr\",\n    \"si_sdr\",\n    \"l1_freq\",\n    \"log_wmse\",\n    \"aura_stft\",\n    \"aura_mrstft\",\n    \"bleedless\",\n    \"fullness\",\n]\n</code></pre>"},{"location":"api/types/#splifft.types.Sdr","title":"Sdr  <code>module-attribute</code>","text":"<pre><code>Sdr: TypeAlias = float\n</code></pre> <p>Signal-to-Distortion Ratio (decibels). Higher is better.</p> <p>Measures the ratio of the power of clean reference signal to the power of all other error components (interference, artifacts, and spatial distortion).</p> <p>Definition: $$ \\text{SDR} = 10 \\log_{10} \\frac{|\\mathbf{s}|^2}{|\\mathbf{s} - \\mathbf{\\hat{s}}|^2}, $$ where:</p> <ul> <li>\\(\\mathbf{s}\\): ground truth source signal</li> <li>\\(\\mathbf{\\hat{s}}\\): estimated source signal produced by the model</li> <li>\\(||\\cdot||^2\\): squared L2 norm (power) of the signal</li> </ul>"},{"location":"api/types/#splifft.types.SiSdr","title":"SiSdr  <code>module-attribute</code>","text":"<pre><code>SiSdr: TypeAlias = float\n</code></pre> <p>Scale-Invariant SDR (SI-SDR) is invariant to scaling errors (decibels). Higher is better.</p> <p>It projects the estimate onto the reference to find the optimal scaling factor \\(\\alpha\\), creating a scaled reference that best matches the estimate's amplitude.</p> <ul> <li>Optimal scaling factor: \\(\\alpha = \\frac{\\langle\\mathbf{\\hat{s}}, \\mathbf{s}\\rangle}{||\\mathbf{s}||^2}\\)</li> <li>Scaled reference: \\(\\mathbf{s}_\\text{target} = \\alpha \\cdot \\mathbf{s}\\)</li> <li>Error: \\(\\mathbf{e} = \\mathbf{\\hat{s}} - \\mathbf{s}_\\text{target}\\)</li> <li>\\(\\text{SI-SDR} = 10 \\log_{10} \\frac{||\\mathbf{s}_\\text{target}||^2}{||\\mathbf{e}||^2}\\)</li> </ul>"},{"location":"api/types/#splifft.types.L1Norm","title":"L1Norm  <code>module-attribute</code>","text":"<pre><code>L1Norm: TypeAlias = float\n</code></pre> <p>L1 norm (mean absolute error) between two signals (dimensionless). Lower is better.</p> <p>Measures the average absolute difference between the reference and estimated signals.</p> <ul> <li>Time domain: \\(\\mathcal{L}_\\text{L1} = \\frac{1}{N} \\sum_{n=1}^{N} |\\mathbf{s}[n] - \\mathbf{\\hat{s}}[n]|\\),</li> <li>Frequency domain: \\(\\mathcal{L}_\\text{L1Freq} = \\frac{1}{\\text{MK}}\\sum_{m=1}^{M} \\sum_{k=1}^{K} \\left||S(m, k)| - |\\hat{S}(m, k)|\\right|\\)</li> </ul>"},{"location":"api/types/#splifft.types.DbDifferenceMel","title":"DbDifferenceMel  <code>module-attribute</code>","text":"<pre><code>DbDifferenceMel: TypeAlias = float\n</code></pre> <p>Difference in the dB-scaled mel spectrogram. $$ \\mathbf{D}(m, k) = \\text{dB}(|\\hat{S}\\text{mel}(m, k)|) - \\text{dB}(|S\\text{mel}(m, k)|) $$</p>"},{"location":"api/types/#splifft.types.Bleedless","title":"Bleedless  <code>module-attribute</code>","text":"<pre><code>Bleedless: TypeAlias = float\n</code></pre> <p>A metric to quantify the amount of \"bleeding\" from other sources. Higher is better.</p> <p>Measures the average energy of the parts of the mel spectrogram that are louder than the reference. A high value indicates that the estimate contains unwanted energy (bleed) from other sources: $$ \\text{Bleed} = \\text{mean}(\\mathbf{D}(m, k)) \\quad \\forall \\quad \\mathbf{D}(m, k) &gt; 0 $$</p>"},{"location":"api/types/#splifft.types.Fullness","title":"Fullness  <code>module-attribute</code>","text":"<pre><code>Fullness: TypeAlias = float\n</code></pre> <p>A metric to quantify how much of the original source is missing. Higher is better.</p> <p>Complementary to Bleedless. Measures the average energy of the parts of the mel spectrogram that are quieter than the reference. A high value indicates that parts of the target loss were lost during the separation, indicating that more of the original source's character is preserved. $$ \\text{Fullness} = \\text{mean}(|\\mathbf{D}(m, k)|) \\quad \\forall \\quad \\mathbf{D}(m, k) &lt; 0 $$</p>"}]}